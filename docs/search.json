[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 5805 Machine Learning",
    "section": "",
    "text": "Anomaly Detection: Credit Card Fraud Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nShrikar Banagiri\n\n\n\n\n\n\n  \n\n\n\n\nClustering of Customer Personality Analysis Data\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nShrikar Banagiri\n\n\n\n\n\n\n  \n\n\n\n\nClassification of Spotify songs into genres\n\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nShrikar Banagiri\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nShrikar Banagiri\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables - Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nShrikar Banagiri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Shrikar Banagiri. I am a PhD student in the Mechanical Engineering department at Virginia Tech. This blog has been created for the CS 5805 Machine Learning project."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Classification of Spotify songs into genres",
    "section": "",
    "text": "This Spotify song dataset contains over 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Classification of Spotify songs into genres",
    "section": "",
    "text": "This Spotify song dataset contains over 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "posts/post-with-code/index.html#importing-the-dataset",
    "href": "posts/post-with-code/index.html#importing-the-dataset",
    "title": "Classification of Spotify songs into genres",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nFirst, we import the libraries required to perform the initial data analysis. The dataset, hosted on Kaggle, is imported and stored in the variable spotify_songs.\n\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs/download?datasetVersionNumber=2' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as spotify_file:\n    spotify_file.extractall(path='datasets')\n    \nspotify_songs = pd.read_csv(Path('datasets/spotify_songs.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "docs/posts/post-with-code/index.html",
    "href": "docs/posts/post-with-code/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/posts/post-with-code/index.html#introduction",
    "href": "docs/posts/post-with-code/index.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nThis Spotify song dataset contains 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "docs/posts/post-with-code/index.html#importing-the-dataset",
    "href": "docs/posts/post-with-code/index.html#importing-the-dataset",
    "title": "",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nFirst, we import the libraries required to perform the initial data analysis. The dataset, hosted on Kaggle, is imported and stored in the variable spotify_songs.\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs/download?datasetVersionNumber=2' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as spotify_file:\n    spotify_file.extractall(path='datasets')\n    \nspotify_songs = pd.read_csv(Path('datasets/spotify_songs.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "posts/post-with-code/index.html#analyzing-the-data",
    "href": "posts/post-with-code/index.html#analyzing-the-data",
    "title": "Classification of Spotify songs into genres",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\nLet’s look at the non-null entries in the dataset using the info() method below.\n\nspotify_songs.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32833 entries, 0 to 32832\nData columns (total 23 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   track_id                  32833 non-null  object \n 1   track_name                32828 non-null  object \n 2   track_artist              32828 non-null  object \n 3   track_popularity          32833 non-null  int64  \n 4   track_album_id            32833 non-null  object \n 5   track_album_name          32828 non-null  object \n 6   track_album_release_date  32833 non-null  object \n 7   playlist_name             32833 non-null  object \n 8   playlist_id               32833 non-null  object \n 9   playlist_genre            32833 non-null  object \n 10  playlist_subgenre         32833 non-null  object \n 11  danceability              32833 non-null  float64\n 12  energy                    32833 non-null  float64\n 13  key                       32833 non-null  int64  \n 14  loudness                  32833 non-null  float64\n 15  mode                      32833 non-null  int64  \n 16  speechiness               32833 non-null  float64\n 17  acousticness              32833 non-null  float64\n 18  instrumentalness          32833 non-null  float64\n 19  liveness                  32833 non-null  float64\n 20  valence                   32833 non-null  float64\n 21  tempo                     32833 non-null  float64\n 22  duration_ms               32833 non-null  int64  \ndtypes: float64(9), int64(4), object(10)\nmemory usage: 5.8+ MB\n\n\nThe output shows that the columns track_name, track_artist, and track_album_name have 5 null elements (i.e., they only have 32828 elements). Let’s look at the genres and their counts in the dataset.\n\nspotify_songs['playlist_genre'].value_counts()\n\nplaylist_genre\nedm      6043\nrap      5746\npop      5507\nr&b      5431\nlatin    5155\nrock     4951\nName: count, dtype: int64\n\n\nLet us now look at how different genres compare on the basis of each attribute. But first, we notice that the features have wildly different scales. For example, loudness varies from -46.448 to 1.275 whereas tempo ranges from 0 to 1 (see histogram below). Therefore, we must standardize these attributes using the StandardScaler. Furthermore, features such as valence and instrumentalness have very long tails. To convert these attributes to “gaussian” like features, we will use the boxcox class from scipy.\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo']]\nspotify_features.hist(bins=50, figsize=(12, 8))\nplt.show()\n\n\n\n\nThe following violin plot also describes the relationship between some of the attributes and genres. From the first violin plot, it is evident that rap music has a higher danceability score on average while the most danceable song belongs to the edm genre.\n\nsns.violinplot(x=spotify_songs.playlist_genre,y=spotify_songs.danceability)\nplt.grid()\nplt.xlabel('Genre')\nplt.ylabel('Danceability score')\nplt.show()\n\n\n\n\nWe should also look for any correlations within the features themselves. Ideally, all our features (i.e., attributes) must be independent of each other. However, it may happen that two or more features are highly correlated within themselves. In such a scenario, we will need to drop one of the correlated attributes.\n\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(spotify_features.corr(), annot=True, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)\nplt.title('Linear Correlations between attributes')\nplt.show()\n\n\n\n\nFrom the correlation matrix, we can note that the attributes energy and loudness are highly correlated. Their correlation coefficient is 0.68. Thus, we will drop the loudness attribute."
  },
  {
    "objectID": "posts/post-with-code/index.html#creating-training-and-test-sets",
    "href": "posts/post-with-code/index.html#creating-training-and-test-sets",
    "title": "Classification of Spotify songs into genres",
    "section": "Creating Training and Test Sets",
    "text": "Creating Training and Test Sets\nNow, we will create training and test sets. We will do this by limiting the test set size to 20 % of the total data set. Since we need an equal distribution of our target labels (i.e., the playlist genres), we will need to stratify the data with respect to the playlist genres.\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(spotify_songs, test_size=0.2, stratify = spotify_songs.playlist_genre,random_state=42)\n\nX_train = train_set.drop('playlist_genre',axis=1)\ny_train = train_set['playlist_genre']\nX_test = test_set.drop('playlist_genre',axis=1)\ny_test = test_set['playlist_genre']"
  },
  {
    "objectID": "posts/post-with-code/index.html#preprocessing-the-data",
    "href": "posts/post-with-code/index.html#preprocessing-the-data",
    "title": "Classification of Spotify songs into genres",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\nWe will first drop the loudness attribute since it is highly correlated with the energy attribute.\n\nspotify_songs = spotify_songs.drop('loudness',axis=1)\n\nThe column playlist_subgenre has attributes such as dance pop and pop edm which are subsets of the parent column playlist_genre. Similarly,the track_id, track_name, track_artist, track_popularity, track_album_id, track_album_name, and track_album_release_date are not relevant. Thus, we drop all these columns. Furthermore, as we have seen before, there are null values in the track_name, track_artist , and track_album_name columns. Therefore, we need to drop the corresponding rows as well.\n\nspotify_songs.dropna(subset=['track_name','track_artist','track_album_name'], inplace=True) # Drop null values from the data\nspotify_songs = spotify_songs.loc[:,'playlist_genre':'tempo'] # drop unnecessary columns\nspotify_songs = spotify_songs.drop('playlist_subgenre',axis=1) # drop unnecessary columns\n\nWe can verify that the null values have been dropped by calling the .info() method again.\n\nspotify_songs.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 32828 entries, 0 to 32832\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   playlist_genre    32828 non-null  object \n 1   danceability      32828 non-null  float64\n 2   energy            32828 non-null  float64\n 3   key               32828 non-null  int64  \n 4   mode              32828 non-null  int64  \n 5   speechiness       32828 non-null  float64\n 6   acousticness      32828 non-null  float64\n 7   instrumentalness  32828 non-null  float64\n 8   liveness          32828 non-null  float64\n 9   valence           32828 non-null  float64\n 10  tempo             32828 non-null  float64\ndtypes: float64(8), int64(2), object(1)\nmemory usage: 3.0+ MB\n\n\nNext, we will use StandardScaler transformer to standardize our attributes. Notice that the spotify_features dataframe in the following code does not include the loudness attribute.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import boxcox\n\nstd_scaler = StandardScaler()\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']] # collecting all the important features in a dataframe\nspotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']] = std_scaler.fit_transform(spotify_features)\n\n# Using boxcox to scale heavy-tailed features\n\nmin_max_scaler = MinMaxScaler()\nspotify_new_features = spotify_songs.loc[:,'speechiness':'liveness']\nspotify_tail_features = min_max_scaler.fit_transform(spotify_new_features)\nspotify_tail_features = pd.DataFrame(spotify_tail_features,columns=spotify_new_features.columns)\ntransformed_acousticness = boxcox(spotify_tail_features.acousticness,0.25) \nspotify_songs.acousticness = transformed_acousticness\ntransformed_liveness = boxcox(spotify_tail_features.acousticness,0.25)\nspotify_songs.liveness = transformed_liveness\ntransformed_instrumentalness = boxcox(spotify_tail_features.instrumentalness,0.2) \nspotify_songs.instrumentalness = transformed_instrumentalness\ntransformed_speechiness = boxcox(spotify_tail_features.speechiness,0.2)\nspotify_songs.speechiness = transformed_speechiness\n\nNow, we can take a look at the feature distributions. As we can see, the attributes are on the same scale and many of them are centered around zero. Most of these distributions are Gaussian.\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']]\nspotify_features.hist(figsize=(12, 8))\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html#train-the-models",
    "href": "posts/post-with-code/index.html#train-the-models",
    "title": "Classification of Spotify songs into genres",
    "section": "Train the models",
    "text": "Train the models\nSince this is a multiclass classification problem, support vector classifiers do not scale very well. Therefore, we will look for other classifiers. First, we’ll try logistic regression.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(max_iter=1000, random_state=42)\nlog_clf.fit(X_train, y_train); # Train the model\n\nTo measure the performance of the classifier, we will first import the cross_val_score function from scikit learn.\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(log_clf,X_train,y_train,cv=3,scoring='accuracy')\n\narray([0.46047521, 0.45042266, 0.45807631])\n\n\nFor the 3 validation folds, Logistic Regression predicts the correct genres with 45 % accuracy on average. Let us compare this performance with a dummy classifier.\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier()\ndummy_clf.fit(X_train, y_train)\n\ncross_val_score(dummy_clf, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.18403016, 0.18414439, 0.18403016])\n\n\nThe dummy classifier predicts the correct genre with 18 % accuracy. The Logistic Regression model is a little better than the dummy classifier for our data. Let us use the DecisionTreeClassifier next. We will use RandomizedSeachCV to select the optimum number of max_features in order to produce the best validation score.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import randint\n\ndecision_tree_clf = DecisionTreeClassifier(random_state=42)\nparam_random = {'max_features':randint(low=2,high=20)}\nrandom_search_tree = RandomizedSearchCV(decision_tree_clf,param_distributions=param_random,n_iter=10,cv=5,scoring='accuracy')\nrandom_search_tree.fit(X_train, y_train)\n\ncross_val_score(random_search_tree, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.41912269, 0.41089788, 0.41181174])\n\n\nThe DecisionTreeClassifier produces an average accuracy of 41 % on the validation folds. We will import the RandomForestClassifier , which aggregates a large number of decision trees together.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1,random_state=42)\nparam_random = [{'n_estimators':randint(low=100, high=500)},{'max_leaf_nodes':randint(low=5,high=20)}]\nrandom_search_forest = RandomizedSearchCV(random_forest_clf,param_distributions=param_random,n_iter=10,cv=3,scoring='accuracy')\nrandom_search_forest.fit(X_train,y_train)\n\ncross_val_score(random_search_forest, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.53301348, 0.53164268, 0.52981494])\n\n\nThe RandomForestClassifier does better than the other models with an average accuracy of 53 % on the validation folds. Let us go one step further by using the AdaBoostClassifier with the RandomForestClassifier as the base estimator with a learning_rate of 0.5.\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nbest_estimator = random_search_forest.best_estimator_\nada_boost = AdaBoostClassifier(best_estimator,n_estimators=5,learning_rate = 0.5, random_state=42)\nada_boost.fit(X_train, y_train)\ncross_val_score(ada_boost, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.53883939, 0.53655472, 0.53769705])\n\n\nThe AdaBoostClassifier actually does slightly worse than the RandomForestClassifier on the validation folds. Therefore, let us use the RandomForestClassifier for the rest of our analysis. Firstly, let us measure the accuracy of the RandomForestClassifier on the test set.\n\nrandom_search_forest.score(X_test, y_test) # print the accuracy score on the test set\n\n0.5325921413341456\n\n\nThis classifier has an accuracy score of 53.7 % on the test set."
  },
  {
    "objectID": "docs/posts/Classification/index.html",
    "href": "docs/posts/Classification/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/posts/Classification/index.html#introduction",
    "href": "docs/posts/Classification/index.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nThis Spotify song dataset contains 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "docs/posts/Classification/index.html#importing-the-dataset",
    "href": "docs/posts/Classification/index.html#importing-the-dataset",
    "title": "",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nFirst, we import the libraries required to perform the initial data analysis. The dataset, hosted on Kaggle, is imported and stored in the variable spotify_songs.\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs/download?datasetVersionNumber=2' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as spotify_file:\n    spotify_file.extractall(path='datasets')\n    \nspotify_songs = pd.read_csv(Path('datasets/spotify_songs.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification of Spotify songs into genres",
    "section": "",
    "text": "This Spotify song dataset contains over 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "posts/Classification/index.html#introduction",
    "href": "posts/Classification/index.html#introduction",
    "title": "Classification of Spotify songs into genres",
    "section": "",
    "text": "This Spotify song dataset contains over 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "posts/Classification/index.html#importing-the-dataset",
    "href": "posts/Classification/index.html#importing-the-dataset",
    "title": "Classification of Spotify songs into genres",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nFirst, we import the libraries required to perform the initial data analysis. The dataset, hosted on Kaggle, is imported and stored in the variable spotify_songs.\n\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs/download?datasetVersionNumber=2' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as spotify_file:\n    spotify_file.extractall(path='datasets')\n    \nspotify_songs = pd.read_csv(Path('datasets/spotify_songs.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "posts/Classification/index.html#analyzing-the-data",
    "href": "posts/Classification/index.html#analyzing-the-data",
    "title": "Classification of Spotify songs into genres",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\nLet’s look at the non-null entries in the dataset using the info() method below.\n\nspotify_songs.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32833 entries, 0 to 32832\nData columns (total 23 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   track_id                  32833 non-null  object \n 1   track_name                32828 non-null  object \n 2   track_artist              32828 non-null  object \n 3   track_popularity          32833 non-null  int64  \n 4   track_album_id            32833 non-null  object \n 5   track_album_name          32828 non-null  object \n 6   track_album_release_date  32833 non-null  object \n 7   playlist_name             32833 non-null  object \n 8   playlist_id               32833 non-null  object \n 9   playlist_genre            32833 non-null  object \n 10  playlist_subgenre         32833 non-null  object \n 11  danceability              32833 non-null  float64\n 12  energy                    32833 non-null  float64\n 13  key                       32833 non-null  int64  \n 14  loudness                  32833 non-null  float64\n 15  mode                      32833 non-null  int64  \n 16  speechiness               32833 non-null  float64\n 17  acousticness              32833 non-null  float64\n 18  instrumentalness          32833 non-null  float64\n 19  liveness                  32833 non-null  float64\n 20  valence                   32833 non-null  float64\n 21  tempo                     32833 non-null  float64\n 22  duration_ms               32833 non-null  int64  \ndtypes: float64(9), int64(4), object(10)\nmemory usage: 5.8+ MB\n\n\nThe output shows that the columns track_name, track_artist, and track_album_name have 5 null elements (i.e., they only have 32828 elements). Let’s look at the genres and their counts in the dataset.\n\nspotify_songs['playlist_genre'].value_counts()\n\nplaylist_genre\nedm      6043\nrap      5746\npop      5507\nr&b      5431\nlatin    5155\nrock     4951\nName: count, dtype: int64\n\n\nLet us now look at how different genres compare on the basis of each attribute. But first, we notice that the features have wildly different scales. For example, loudness varies from -46.448 to 1.275 whereas tempo ranges from 0 to 1 (see histogram below). Therefore, we must standardize these attributes using the StandardScaler. Furthermore, features such as valence and instrumentalness have very long tails. To convert these attributes to “gaussian” like features, we will use the boxcox class from scipy.\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo']]\nspotify_features.hist(bins=50, figsize=(12, 8))\nplt.show()\n\n\n\n\nThe following violin plot also describes the relationship between some of the attributes and genres. From the first violin plot, it is evident that rap music has a higher danceability score on average while the most danceable song belongs to the edm genre.\n\nsns.violinplot(x=spotify_songs.playlist_genre,y=spotify_songs.danceability)\nplt.grid()\nplt.xlabel('Genre')\nplt.ylabel('Danceability score')\nplt.show()\n\n\n\n\nWe should also look for any correlations within the features themselves. Ideally, all our features (i.e., attributes) must be independent of each other. However, it may happen that two or more features are highly correlated within themselves. In such a scenario, we will need to drop one of the correlated attributes.\n\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(spotify_features.corr(), annot=True, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)\nplt.title('Linear Correlations between attributes')\nplt.show()\n\n\n\n\nFrom the correlation matrix, we can note that the attributes energy and loudness are highly correlated. Their correlation coefficient is 0.68. Thus, we will drop the loudness attribute."
  },
  {
    "objectID": "posts/Classification/index.html#preprocessing-the-data",
    "href": "posts/Classification/index.html#preprocessing-the-data",
    "title": "Classification of Spotify songs into genres",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\nWe will first drop the loudness attribute since it is highly correlated with the energy attribute.\n\nspotify_songs = spotify_songs.drop('loudness',axis=1)\n\nThe column playlist_subgenre has attributes such as dance pop and pop edm which are subsets of the parent column playlist_genre. Similarly,the track_id, track_name, track_artist, track_popularity, track_album_id, track_album_name, and track_album_release_date are not relevant. Thus, we drop all these columns. Furthermore, as we have seen before, there are null values in the track_name, track_artist , and track_album_name columns. Therefore, we need to drop the corresponding rows as well.\n\nspotify_songs.dropna(subset=['track_name','track_artist','track_album_name'], inplace=True) # Drop null values from the data\nspotify_songs = spotify_songs.loc[:,'playlist_genre':'tempo'] # drop unnecessary columns\nspotify_songs = spotify_songs.drop('playlist_subgenre',axis=1) # drop unnecessary columns\n\nWe can verify that the null values have been dropped by calling the .info() method again.\n\nspotify_songs.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 32828 entries, 0 to 32832\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   playlist_genre    32828 non-null  object \n 1   danceability      32828 non-null  float64\n 2   energy            32828 non-null  float64\n 3   key               32828 non-null  int64  \n 4   mode              32828 non-null  int64  \n 5   speechiness       32828 non-null  float64\n 6   acousticness      32828 non-null  float64\n 7   instrumentalness  32828 non-null  float64\n 8   liveness          32828 non-null  float64\n 9   valence           32828 non-null  float64\n 10  tempo             32828 non-null  float64\ndtypes: float64(8), int64(2), object(1)\nmemory usage: 3.0+ MB\n\n\nNext, we will use StandardScaler transformer to standardize our attributes. Notice that the spotify_features dataframe in the following code does not include the loudness attribute.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import boxcox\n\nstd_scaler = StandardScaler()\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']] # collecting all the important features in a dataframe\nspotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']] = std_scaler.fit_transform(spotify_features)\n\n# Using boxcox to scale heavy-tailed features\n\nmin_max_scaler = MinMaxScaler()\nspotify_new_features = spotify_songs.loc[:,'speechiness':'liveness']\nspotify_tail_features = min_max_scaler.fit_transform(spotify_new_features)\nspotify_tail_features = pd.DataFrame(spotify_tail_features,columns=spotify_new_features.columns)\ntransformed_acousticness = boxcox(spotify_tail_features.acousticness,0.25) \nspotify_songs.acousticness = transformed_acousticness\ntransformed_liveness = boxcox(spotify_tail_features.acousticness,0.25)\nspotify_songs.liveness = transformed_liveness\ntransformed_instrumentalness = boxcox(spotify_tail_features.instrumentalness,0.2) \nspotify_songs.instrumentalness = transformed_instrumentalness\ntransformed_speechiness = boxcox(spotify_tail_features.speechiness,0.2)\nspotify_songs.speechiness = transformed_speechiness\n\nNow, we can take a look at the feature distributions. As we can see, the attributes are on the same scale and many of them are centered around zero. Most of these distributions are Gaussian.\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']]\nspotify_features.hist(figsize=(12, 8))\nplt.show()"
  },
  {
    "objectID": "posts/Classification/index.html#creating-training-and-test-sets",
    "href": "posts/Classification/index.html#creating-training-and-test-sets",
    "title": "Classification of Spotify songs into genres",
    "section": "Creating Training and Test Sets",
    "text": "Creating Training and Test Sets\nNow, we will create training and test sets. We will do this by limiting the test set size to 20 % of the total data set. Since we need an equal distribution of our target labels (i.e., the playlist genres), we will need to stratify the data with respect to the playlist genres.\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(spotify_songs, test_size=0.2, stratify = spotify_songs.playlist_genre,random_state=42)\n\nX_train = train_set.drop('playlist_genre',axis=1)\ny_train = train_set['playlist_genre']\nX_test = test_set.drop('playlist_genre',axis=1)\ny_test = test_set['playlist_genre']"
  },
  {
    "objectID": "posts/Classification/index.html#train-the-models",
    "href": "posts/Classification/index.html#train-the-models",
    "title": "Classification of Spotify songs into genres",
    "section": "Train the models",
    "text": "Train the models\nSince this is a multiclass classification problem, support vector classifiers do not scale very well. Therefore, we will look for other classifiers. First, we’ll try logistic regression.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(max_iter=1000, random_state=42)\nlog_clf.fit(X_train, y_train); # Train the model\n\nTo measure the performance of the classifier, we will first import the cross_val_score function from scikit learn.\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(log_clf,X_train,y_train,cv=3,scoring='accuracy')\n\narray([0.46047521, 0.45042266, 0.45807631])\n\n\nFor the 3 validation folds, Logistic Regression predicts the correct genres with 45 % accuracy on average. Let us compare this performance with a dummy classifier.\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier()\ndummy_clf.fit(X_train, y_train)\n\ncross_val_score(dummy_clf, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.18403016, 0.18414439, 0.18403016])\n\n\nThe dummy classifier predicts the correct genre with 18 % accuracy. The Logistic Regression model is a little better than the dummy classifier for our data. Let us use the DecisionTreeClassifier next. We will use RandomizedSeachCV to select the optimum number of max_features in order to produce the best validation score.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import randint\n\ndecision_tree_clf = DecisionTreeClassifier(random_state=42)\nparam_random = {'max_features':randint(low=2,high=20)}\nrandom_search_tree = RandomizedSearchCV(decision_tree_clf,param_distributions=param_random,n_iter=10,cv=5,scoring='accuracy')\nrandom_search_tree.fit(X_train, y_train)\n\ncross_val_score(random_search_tree, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.41912269, 0.41089788, 0.41181174])\n\n\nThe DecisionTreeClassifier produces an average accuracy of 41 % on the validation folds. We will import the RandomForestClassifier , which aggregates a large number of decision trees together.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1,random_state=42)\nparam_random = [{'n_estimators':randint(low=100, high=500)},{'max_leaf_nodes':randint(low=5,high=20)}]\nrandom_search_forest = RandomizedSearchCV(random_forest_clf,param_distributions=param_random,n_iter=10,cv=3,scoring='accuracy')\nrandom_search_forest.fit(X_train,y_train)\n\ncross_val_score(random_search_forest, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.53301348, 0.53164268, 0.52981494])\n\n\nThe RandomForestClassifier does better than the other models with an average accuracy of 53 % on the validation folds. Let us go one step further by using the AdaBoostClassifier with the RandomForestClassifier as the base estimator with a learning_rate of 0.5.\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nbest_estimator = random_search_forest.best_estimator_\nada_boost = AdaBoostClassifier(best_estimator,n_estimators=5,learning_rate = 0.5, random_state=42)\nada_boost.fit(X_train, y_train)\ncross_val_score(ada_boost, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.53883939, 0.53655472, 0.53769705])\n\n\nThe AdaBoostClassifier actually does slightly worse than the RandomForestClassifier on the validation folds. Therefore, let us use the RandomForestClassifier for the rest of our analysis. Firstly, let us measure the accuracy of the RandomForestClassifier on the test set.\n\nrandom_search_forest.score(X_test, y_test) # print the accuracy score on the test set\n\n0.5325921413341456\n\n\nThis classifier has an accuracy score of 53.7 % on the test set."
  },
  {
    "objectID": "posts/Classification/index.html#error-analysis",
    "href": "posts/Classification/index.html#error-analysis",
    "title": "Classification of Spotify songs into genres",
    "section": "Error Analysis",
    "text": "Error Analysis\nThe code below produces a Confusion Matrix for the RandomForestClassifier. As shown in the confusion matrix, many edm tracks (14 %) have been misclassified as pop. 16 % of the latin tracks have been misclassified as pop and rap each. Similarly, 17 % of pop tracks have been misclassified as edm and r&b each. 21 % of r&b tracks have been misclassified. These misclassifications have been caused due to similarities in the genres themselves. For instance, many r&b and rap songs have similarity danceability and speechiness. Further data collection is needed to differentiate the genres adequately.\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_preds = cross_val_predict(random_search_forest.best_estimator_,X_train, y_train, cv=3)\n\nConfusionMatrixDisplay.from_predictions(y_train,y_preds,normalize='true',values_format='.0%',colorbar=None) # normalize predictions by row\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html#error-analysis",
    "href": "posts/post-with-code/index.html#error-analysis",
    "title": "Classification of Spotify songs into genres",
    "section": "Error Analysis",
    "text": "Error Analysis\nThe code below produces a Confusion Matrix for the RandomForestClassifier. As shown in the confusion matrix, many edm tracks (14 %) have been misclassified as pop. 16 % of the latin tracks have been misclassified as pop and rap each. Similarly, 17 % of pop tracks have been misclassified as edm and r&b each. 21 % of r&b tracks have been misclassified. These misclassifications have been caused due to similarities in the genres themselves. For instance, many r&b and rap songs have similarity danceability and speechiness. Further data collection is needed to differentiate the genres adequately.\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_preds = cross_val_predict(random_search_forest.best_estimator_,X_train, y_train, cv=3)\n\nConfusionMatrixDisplay.from_predictions(y_train,y_preds,normalize='true',values_format='.0%',colorbar=None) # normalize predictions by row\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "The Bike Sharing Dataset compiles the hourly and daily counts of rental bikes between 2011 and 2012. There are two .csv files: hour.csv and day.csv. These files have the following 15 features:\n\ndteday: The date.\nseason: season (1 represents winter, 2 represents spring, 3 represents summer, 4 represents fall).\nyr: year (0 represents 2011, 1 represents 2012).\nmnth: month (1 to 12).\nhr: hour (0 to 23).\nholiday: whether day is a holiday or not (0 represents not a holiday, 1 represents a holiday).\nweekday: the day of the week (0 for Sunday and 6 Saturday).\nworkingday: if working day, the value is 1; else the value is 0.\nweathersit: 1 represents clear weather, 2 represents mist / mist + broken clouds / mist + few clouds, 3 represents light snow / light rain, 4 represents heavy rain + thunderstorm + ice pallets / snow + fog.\ntemp: this represents the normalized temperature in \\(^{\\circ}\\)C.\natemp: this represents the normalized “feels like” temperature in \\(^{\\circ}\\)C.\nhum: Normalized humidity, ranges from 0 - 1.\nwindspeed: Normalized windspeed, ranges from 0 - 1.\ncasual: the number of casual bikers.\nregistered: the number of registered bikers.\ncnts: the total number of casual and registered bikes.\n\nThe problem statement is to predict the total number of both casual and registered bikes cnts given the values of the rest of the features."
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#introduction",
    "href": "posts/Linear and Nonlinear Regression/index.html#introduction",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "The Bike Sharing Dataset compiles the hourly and daily counts of rental bikes between 2011 and 2012. There are two .csv files: hour.csv and day.csv. These files have the following 15 features:\n\ndteday: The date.\nseason: season (1 represents winter, 2 represents spring, 3 represents summer, 4 represents fall).\nyr: year (0 represents 2011, 1 represents 2012).\nmnth: month (1 to 12).\nhr: hour (0 to 23).\nholiday: whether day is a holiday or not (0 represents not a holiday, 1 represents a holiday).\nweekday: the day of the week (0 for Sunday and 6 Saturday).\nworkingday: if working day, the value is 1; else the value is 0.\nweathersit: 1 represents clear weather, 2 represents mist / mist + broken clouds / mist + few clouds, 3 represents light snow / light rain, 4 represents heavy rain + thunderstorm + ice pallets / snow + fog.\ntemp: this represents the normalized temperature in \\(^{\\circ}\\)C.\natemp: this represents the normalized “feels like” temperature in \\(^{\\circ}\\)C.\nhum: Normalized humidity, ranges from 0 - 1.\nwindspeed: Normalized windspeed, ranges from 0 - 1.\ncasual: the number of casual bikers.\nregistered: the number of registered bikers.\ncnts: the total number of casual and registered bikes.\n\nThe problem statement is to predict the total number of both casual and registered bikes cnts given the values of the rest of the features."
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html",
    "href": "posts/Classification of spotify songs/index.html",
    "title": "Classification of Spotify songs into genres",
    "section": "",
    "text": "This Spotify song dataset contains over 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html#introduction",
    "href": "posts/Classification of spotify songs/index.html#introduction",
    "title": "Classification of Spotify songs into genres",
    "section": "",
    "text": "This Spotify song dataset contains over 30,000 songs from artists of different genres. Each song has specific attributes which help the user discern its genre. These attributes include danceability, energy, loudness, key, mode, speechiness, acousticness, instrumentalness, liveness, valence, and tempo.\nThe problem statement is to classify the songs (based on the above attributes) into six genres: pop, rock, latin, EDM, rap, and R&B."
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html#importing-the-dataset",
    "href": "posts/Classification of spotify songs/index.html#importing-the-dataset",
    "title": "Classification of Spotify songs into genres",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nFirst, we import the libraries required to perform the initial data analysis. The dataset, hosted on Kaggle, is imported and stored in the variable spotify_songs.\n\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs/download?datasetVersionNumber=2' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as spotify_file:\n    spotify_file.extractall(path='datasets')\n    \nspotify_songs = pd.read_csv(Path('datasets/spotify_songs.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html#analyzing-the-data",
    "href": "posts/Classification of spotify songs/index.html#analyzing-the-data",
    "title": "Classification of Spotify songs into genres",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\nLet’s look at the non-null entries in the dataset using the info() method below.\n\nspotify_songs.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32833 entries, 0 to 32832\nData columns (total 23 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   track_id                  32833 non-null  object \n 1   track_name                32828 non-null  object \n 2   track_artist              32828 non-null  object \n 3   track_popularity          32833 non-null  int64  \n 4   track_album_id            32833 non-null  object \n 5   track_album_name          32828 non-null  object \n 6   track_album_release_date  32833 non-null  object \n 7   playlist_name             32833 non-null  object \n 8   playlist_id               32833 non-null  object \n 9   playlist_genre            32833 non-null  object \n 10  playlist_subgenre         32833 non-null  object \n 11  danceability              32833 non-null  float64\n 12  energy                    32833 non-null  float64\n 13  key                       32833 non-null  int64  \n 14  loudness                  32833 non-null  float64\n 15  mode                      32833 non-null  int64  \n 16  speechiness               32833 non-null  float64\n 17  acousticness              32833 non-null  float64\n 18  instrumentalness          32833 non-null  float64\n 19  liveness                  32833 non-null  float64\n 20  valence                   32833 non-null  float64\n 21  tempo                     32833 non-null  float64\n 22  duration_ms               32833 non-null  int64  \ndtypes: float64(9), int64(4), object(10)\nmemory usage: 5.8+ MB\n\n\nThe output shows that the columns track_name, track_artist, and track_album_name have 5 null elements (i.e., they only have 32828 elements). Let’s look at the genres and their counts in the dataset.\n\nspotify_songs['playlist_genre'].value_counts()\n\nplaylist_genre\nedm      6043\nrap      5746\npop      5507\nr&b      5431\nlatin    5155\nrock     4951\nName: count, dtype: int64\n\n\nLet us now look at how different genres compare on the basis of each attribute. But first, we notice that the features have wildly different scales. For example, loudness varies from -46.448 to 1.275 whereas tempo ranges from 0 to 1 (see histogram below). Therefore, we must standardize these attributes using the StandardScaler. Furthermore, features such as valence and instrumentalness have very long tails. To convert these attributes to “gaussian” like features, we will use the boxcox class from scipy.\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo']]\nspotify_features.hist(bins=50, figsize=(12, 8))\nplt.show()\n\n\n\n\nThe following violin plot also describes the relationship between some of the attributes and genres. From the first violin plot, it is evident that rap music has a higher danceability score on average while the most danceable song belongs to the edm genre.\n\nsns.violinplot(x=spotify_songs.playlist_genre,y=spotify_songs.danceability)\nplt.grid()\nplt.xlabel('Genre')\nplt.ylabel('Danceability score')\nplt.show()\n\n\n\n\nWe should also look for any correlations within the features themselves. Ideally, all our features (i.e., attributes) must be independent of each other. However, it may happen that two or more features are highly correlated within themselves. In such a scenario, we will need to drop one of the correlated attributes.\n\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(spotify_features.corr(), annot=True, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)\nplt.title('Linear Correlations between attributes')\nplt.show()\n\n\n\n\nFrom the correlation matrix, we can note that the attributes energy and loudness are highly correlated. Their correlation coefficient is 0.68. Thus, we will drop the loudness attribute."
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html#preprocessing-the-data",
    "href": "posts/Classification of spotify songs/index.html#preprocessing-the-data",
    "title": "Classification of Spotify songs into genres",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\nWe will first drop the loudness attribute since it is highly correlated with the energy attribute.\n\nspotify_songs = spotify_songs.drop('loudness',axis=1)\n\nThe column playlist_subgenre has attributes such as dance pop and pop edm which are subsets of the parent column playlist_genre. Similarly,the track_id, track_name, track_artist, track_popularity, track_album_id, track_album_name, and track_album_release_date are not relevant. Thus, we drop all these columns. Furthermore, as we have seen before, there are null values in the track_name, track_artist , and track_album_name columns. Therefore, we need to drop the corresponding rows as well.\n\nspotify_songs.dropna(subset=['track_name','track_artist','track_album_name'], inplace=True) # Drop null values from the data\nspotify_songs = spotify_songs.loc[:,'playlist_genre':'tempo'] # drop unnecessary columns\nspotify_songs = spotify_songs.drop('playlist_subgenre',axis=1) # drop unnecessary columns\n\nWe can verify that the null values have been dropped by calling the .info() method again.\n\nspotify_songs.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 32828 entries, 0 to 32832\nData columns (total 11 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   playlist_genre    32828 non-null  object \n 1   danceability      32828 non-null  float64\n 2   energy            32828 non-null  float64\n 3   key               32828 non-null  int64  \n 4   mode              32828 non-null  int64  \n 5   speechiness       32828 non-null  float64\n 6   acousticness      32828 non-null  float64\n 7   instrumentalness  32828 non-null  float64\n 8   liveness          32828 non-null  float64\n 9   valence           32828 non-null  float64\n 10  tempo             32828 non-null  float64\ndtypes: float64(8), int64(2), object(1)\nmemory usage: 3.0+ MB\n\n\nNext, we will use StandardScaler transformer to standardize our attributes. Notice that the spotify_features dataframe in the following code does not include the loudness attribute.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import boxcox\n\nstd_scaler = StandardScaler()\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']] # collecting all the important features in a dataframe\nspotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']] = std_scaler.fit_transform(spotify_features)\n\n# Using boxcox to scale heavy-tailed features\n\nmin_max_scaler = MinMaxScaler()\nspotify_new_features = spotify_songs.loc[:,'speechiness':'liveness']\nspotify_tail_features = min_max_scaler.fit_transform(spotify_new_features)\nspotify_tail_features = pd.DataFrame(spotify_tail_features,columns=spotify_new_features.columns)\ntransformed_acousticness = boxcox(spotify_tail_features.acousticness,0.25) \nspotify_songs.acousticness = transformed_acousticness\ntransformed_liveness = boxcox(spotify_tail_features.acousticness,0.25)\nspotify_songs.liveness = transformed_liveness\ntransformed_instrumentalness = boxcox(spotify_tail_features.instrumentalness,0.2) \nspotify_songs.instrumentalness = transformed_instrumentalness\ntransformed_speechiness = boxcox(spotify_tail_features.speechiness,0.2)\nspotify_songs.speechiness = transformed_speechiness\n\nNow, we can take a look at the feature distributions. As we can see, the attributes are on the same scale and many of them are centered around zero. Most of these distributions are Gaussian.\n\nspotify_features = spotify_songs.loc[:,['danceability','energy','speechiness','acousticness','instrumentalness','liveness','valence','tempo']]\nspotify_features.hist(figsize=(12, 8))\nplt.show()"
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html#creating-training-and-test-sets",
    "href": "posts/Classification of spotify songs/index.html#creating-training-and-test-sets",
    "title": "Classification of Spotify songs into genres",
    "section": "Creating Training and Test Sets",
    "text": "Creating Training and Test Sets\nNow, we will create training and test sets. We will do this by limiting the test set size to 20 % of the total data set. Since we need an equal distribution of our target labels (i.e., the playlist genres), we will need to stratify the data with respect to the playlist genres.\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(spotify_songs, test_size=0.2, stratify = spotify_songs.playlist_genre,random_state=42)\n\nX_train = train_set.drop('playlist_genre',axis=1)\ny_train = train_set['playlist_genre']\nX_test = test_set.drop('playlist_genre',axis=1)\ny_test = test_set['playlist_genre']"
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html#train-the-models",
    "href": "posts/Classification of spotify songs/index.html#train-the-models",
    "title": "Classification of Spotify songs into genres",
    "section": "Train the models",
    "text": "Train the models\nSince this is a multiclass classification problem, support vector classifiers do not scale very well. Therefore, we will look for other classifiers. First, we’ll try logistic regression.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(max_iter=1000, random_state=42)\nlog_clf.fit(X_train, y_train); # Train the model\n\nTo measure the performance of the classifier, we will first import the cross_val_score function from scikit learn.\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(log_clf,X_train,y_train,cv=3,scoring='accuracy')\n\narray([0.46047521, 0.45042266, 0.45807631])\n\n\nFor the 3 validation folds, Logistic Regression predicts the correct genres with 45 % accuracy on average. Let us compare this performance with a dummy classifier.\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier()\ndummy_clf.fit(X_train, y_train)\n\ncross_val_score(dummy_clf, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.18403016, 0.18414439, 0.18403016])\n\n\nThe dummy classifier predicts the correct genre with 18 % accuracy. The Logistic Regression model is a little better than the dummy classifier for our data. Let us use the DecisionTreeClassifier next. We will use RandomizedSeachCV to select the optimum number of max_features in order to produce the best validation score.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import randint\n\ndecision_tree_clf = DecisionTreeClassifier(random_state=42)\nparam_random = {'max_features':randint(low=2,high=20)}\nrandom_search_tree = RandomizedSearchCV(decision_tree_clf,param_distributions=param_random,n_iter=10,cv=5,scoring='accuracy')\nrandom_search_tree.fit(X_train, y_train)\n\ncross_val_score(random_search_tree, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.41980809, 0.41089788, 0.41649532])\n\n\nThe DecisionTreeClassifier produces an average accuracy of 41 % on the validation folds. We will import the RandomForestClassifier , which aggregates a large number of decision trees together.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest_clf = RandomForestClassifier(n_jobs=-1,random_state=42)\nparam_random = [{'n_estimators':randint(low=100, high=500)},{'max_leaf_nodes':randint(low=5,high=20)}]\nrandom_search_forest = RandomizedSearchCV(random_forest_clf,param_distributions=param_random,n_iter=10,cv=3,scoring='accuracy')\nrandom_search_forest.fit(X_train,y_train)\n\ncross_val_score(random_search_forest, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.53312771, 0.53152844, 0.52981494])\n\n\nThe RandomForestClassifier does better than the other models with an average accuracy of 53 % on the validation folds. Let us go one step further by using the AdaBoostClassifier with the RandomForestClassifier as the base estimator with a learning_rate of 0.5.\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nbest_estimator = random_search_forest.best_estimator_\nada_boost = AdaBoostClassifier(best_estimator,n_estimators=5,learning_rate = 0.5, random_state=42)\nada_boost.fit(X_train, y_train)\ncross_val_score(ada_boost, X_train, y_train, cv=3,scoring='accuracy')\n\narray([0.53518392, 0.53278501, 0.53689742])\n\n\nThe AdaBoostClassifier actually does slightly worse than the RandomForestClassifier on the validation folds. Therefore, let us use the RandomForestClassifier for the rest of our analysis. Firstly, let us measure the accuracy of the RandomForestClassifier on the test set.\n\nrandom_search_forest.score(X_test, y_test) # print the accuracy score on the test set\n\n0.5275662503807493\n\n\nThis classifier has an accuracy score of 53.7 % on the test set."
  },
  {
    "objectID": "posts/Classification of spotify songs/index.html#error-analysis",
    "href": "posts/Classification of spotify songs/index.html#error-analysis",
    "title": "Classification of Spotify songs into genres",
    "section": "Error Analysis",
    "text": "Error Analysis\nThe code below produces a Confusion Matrix for the RandomForestClassifier. As shown in the confusion matrix, many edm tracks (14 %) have been misclassified as pop. 16 % of the latin tracks have been misclassified as pop and rap each. Similarly, 17 % of pop tracks have been misclassified as edm and r&b each. 21 % of r&b tracks have been misclassified. These misclassifications have been caused due to similarities in the genres themselves. For instance, many r&b and rap songs have similarity danceability and speechiness. Further data collection is needed to differentiate the genres adequately.\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_preds = cross_val_predict(random_search_forest.best_estimator_,X_train, y_train, cv=3)\n\nConfusionMatrixDisplay.from_predictions(y_train,y_preds,normalize='true',values_format='.0%',colorbar=None) # normalize predictions by row\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#importing-the-dataset",
    "href": "posts/Linear and Nonlinear Regression/index.html#importing-the-dataset",
    "title": "Linear and Nonlinear Regression",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nThe dataset is hosted on the UC Irvine Machine Learning Repository. First, we import the libraries required to perform the analysis.\n\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/bike+sharing+dataset.zip') # store the dataset in a local folder\nurl = 'https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as bike_file:\n    bike_file.extractall(path='datasets')\n    \nbike_sharing_day = pd.read_csv(Path('datasets/day.csv')) \nbike_sharing_hour = pd.read_csv(Path('datasets/hour.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#analyzing-the-data",
    "href": "posts/Linear and Nonlinear Regression/index.html#analyzing-the-data",
    "title": "Linear and Nonlinear Regression",
    "section": "Analyzing the data",
    "text": "Analyzing the data\nLet us survey the data by using the info() method below. As we can see, there are no null entries in this dataset.\n\nbike_sharing_hour.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17379 entries, 0 to 17378\nData columns (total 17 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   instant     17379 non-null  int64  \n 1   dteday      17379 non-null  object \n 2   season      17379 non-null  int64  \n 3   yr          17379 non-null  int64  \n 4   mnth        17379 non-null  int64  \n 5   hr          17379 non-null  int64  \n 6   holiday     17379 non-null  int64  \n 7   weekday     17379 non-null  int64  \n 8   workingday  17379 non-null  int64  \n 9   weathersit  17379 non-null  int64  \n 10  temp        17379 non-null  float64\n 11  atemp       17379 non-null  float64\n 12  hum         17379 non-null  float64\n 13  windspeed   17379 non-null  float64\n 14  casual      17379 non-null  int64  \n 15  registered  17379 non-null  int64  \n 16  cnt         17379 non-null  int64  \ndtypes: float64(4), int64(12), object(1)\nmemory usage: 2.3+ MB\n\n\nLet us now look at the counts of the individual dates.\n\nbike_sharing_hour['dteday'].value_counts()\n\ndteday\n2011-01-01    24\n2012-04-03    24\n2012-04-28    24\n2012-04-29    24\n2012-04-30    24\n              ..\n2011-01-26    16\n2011-01-18    12\n2012-10-30    11\n2011-01-27     8\n2012-10-29     1\nName: count, Length: 731, dtype: int64\n\n\nAs shown in the value counts of the dates, not all hours of all dates are recorded. For example, on 29 October 2012, data for only one hour is available. Furthermore, we need to convert the dteday column into a datetime format. Furthermore, we will split the dates into separate columns comprising days, months, and years.\n\nbike_sharing_hour['dteday'] = pd.to_datetime(bike_sharing_hour['dteday']) # convert to date time format\nbike_sharing_hour['day'] = bike_sharing_hour['dteday'].dt.day # split into days\nbike_sharing_hour = bike_sharing_hour.drop('dteday',axis=1) # Drop the dteday column\n\nNext, drop the irrelevant and redundant features. The instant column just represents an index, atemp is just a modification of the temp feature, casual and rental are just subsets of the cnt feature. holiday and weekday are together described by the feature workingday. Thus, we will drop all these features.\n\nbike_sharing_hour = bike_sharing_hour.drop(['instant','holiday','weekday','atemp','casual','registered'],axis=1) # drop the irrelevant columns\n\nLet us now look at the correlation matrix for this dataset.\n\ncorr_matrix = bike_sharing_hour.corr() # correlation matrix\ncorr_matrix['cnt'].sort_values(ascending=False) # see the linear correlation values for the counts feature\n\ncnt           1.000000\ntemp          0.404772\nhr            0.394071\nyr            0.250495\nseason        0.178056\nmnth          0.120638\nwindspeed     0.093234\nworkingday    0.030284\nday          -0.004312\nweathersit   -0.142426\nhum          -0.322911\nName: cnt, dtype: float64\n\n\nAs shown in the matrix, the day feature has a very weak correlation with the cnt feature. Thus, we can drop this feature\n\nbike_sharing_hour = bike_sharing_hour.drop('day',axis=1)\n\nLet us look at the correlation matrix for our dataframe now.\n\ncorr = bike_sharing_hour.corr() # correlation matrix\nsns.heatmap(corr,cmap='coolwarm')\nplt.title('Linear Correlations between attributes')\nplt.show()\n\n\n\n\nFrom the correlation matrix, we can see that the features season and mnth are highly correlated with each other. Thus, we can drop one of these features. We will drop the mnth feature. Furthermore, as shown in the violinplot below, season is very well correlated with the cnt . Season 3 (Summer) has the highest number of median bike rentals.\n\nbike_sharing_hour = bike_sharing_hour.drop('mnth',axis=1) # drop the month feature from the dataset\n\n\n\n\n\n\nSince we have categorical attributes in our dataset, we will need to do one hot encoding as shown below.\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nbike_cats = bike_sharing_hour[['season','weathersit']] # categorical columns segregation\ncat_array_encoded = cat_encoder.fit_transform(bike_cats)\nbike_cats_encoded = pd.DataFrame(cat_array_encoded.toarray(),columns=cat_encoder.get_feature_names_out()) # encoded dataframe\n\nframes = [bike_sharing_hour, bike_cats_encoded]\nbike_sharing_hour_encoded = pd.concat(frames,axis=1) # encode the categorical variable in the original data frame\nbike_sharing_hour_encoded = bike_sharing_hour_encoded.drop(['season','weathersit'],axis=1) # drop redundant columns\n\nLet us look at the distributions of the non-categorical features in the dataset.\n\nbike_features = bike_sharing_hour_encoded[['temp','hum','windspeed']] # non-categorical features in the DataFrame\nbike_features.hist(bins=50) # histogram\n\narray([[&lt;Axes: title={'center': 'temp'}&gt;,\n        &lt;Axes: title={'center': 'hum'}&gt;],\n       [&lt;Axes: title={'center': 'windspeed'}&gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\nAs we can see from the distributions, the temp feature has a gaussian like distribution whereas hum and windspeed are not gaussian. Thus, we will use the StandardScaler and boxcox transformer were used to transform these features.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import boxcox\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler(feature_range=(1,2))\nstd_scaler = StandardScaler()\n\nbike_sharing_hour_encoded[['windspeed']] = min_max_scaler.fit_transform(bike_sharing_hour_encoded[['windspeed']])\nbike_sharing_hour_encoded[['windspeed']] = boxcox(bike_sharing_hour_encoded[['windspeed']],0)\nbike_sharing_hour_encoded[['hum']] = boxcox(bike_sharing_hour_encoded[['hum']],0.5) # boxcox transform to scale hum feature\nbike_sharing_hour_encoded[['temp']] = std_scaler.fit_transform(bike_sharing_hour_encoded[['temp']])\nbike_sharing_hour_encoded[['hum']] = std_scaler.fit_transform(bike_sharing_hour_encoded[['hum']])\nbike_sharing_hour_encoded[['windspeed']] = std_scaler.fit_transform(bike_sharing_hour_encoded[['windspeed']])# Standardize the features\n\nbike_features = bike_sharing_hour_encoded[['temp','hum','windspeed']]\nbike_features.hist(bins=50) # histogram\n\narray([[&lt;Axes: title={'center': 'temp'}&gt;,\n        &lt;Axes: title={'center': 'hum'}&gt;],\n       [&lt;Axes: title={'center': 'windspeed'}&gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\nThe histograms now show a fairly gaussian distribution except for a few outliers in the hum feature. Let us remove these outliers.\n\nhum_outliers = bike_sharing_hour_encoded[bike_sharing_hour_encoded.hum == -6.0] # outliers in the humidity features\nbike_new = bike_sharing_hour_encoded.merge(hum_outliers,how='left',indicator=True) \nbike_new = bike_new[bike_new['_merge']=='left_only']\n\nbike_new = bike_new.drop(['_merge'],axis=1)\nbike_sharing_hour_encoded = bike_new # removed humidity outliers\n\nbike_features = bike_sharing_hour_encoded[['temp','hum','windspeed']]\nbike_features.hist(bins=50) # plot histogram again\n\narray([[&lt;Axes: title={'center': 'temp'}&gt;,\n        &lt;Axes: title={'center': 'hum'}&gt;],\n       [&lt;Axes: title={'center': 'windspeed'}&gt;, &lt;Axes: &gt;]], dtype=object)"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#creating-training-and-test-seta",
    "href": "posts/Linear and Nonlinear Regression/index.html#creating-training-and-test-seta",
    "title": "Linear and Nonlinear Regression",
    "section": "Creating Training and Test seta",
    "text": "Creating Training and Test seta"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#creating-training-and-test-sets",
    "href": "posts/Linear and Nonlinear Regression/index.html#creating-training-and-test-sets",
    "title": "Linear and Nonlinear Regression",
    "section": "Creating Training and Test sets",
    "text": "Creating Training and Test sets\nNow, let us create the training and test sets as shown below.\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(bike_sharing_hour_encoded,test_size=0.2,random_state=42) # split the training and test sets\n\nX_train = train_set.drop('cnt',axis=1) # drop the target label from the training set\ny_train = train_set['cnt'] # target label\nX_test = test_set.drop('cnt',axis=1) # drop the target label from the test set\ny_test = test_set['cnt'] # target label"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#training-the-models",
    "href": "posts/Linear and Nonlinear Regression/index.html#training-the-models",
    "title": "Linear and Nonlinear Regression",
    "section": "Training the models",
    "text": "Training the models\nFor this regression problem, let us use linear regression and non-linear regression models. Let us first try the simplest regression, the LinearRegression.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nlin_reg = LinearRegression() \nlin_reg.fit(X_train,y_train) # Train the model\n\n-cross_val_score(lin_reg,X_train,y_train,cv=5,scoring='neg_root_mean_squared_error')\n\narray([138.25778269, 142.84438603, 139.61695447, 146.46972741,\n       141.99234977])\n\n\nFor five-fold cross validation, LinearRegression produces a root mean squared error (RMSE) of about 140 on average. Let us look at the learning curve to see if the model is overfitting or underfitting the training data.\n\n# import learning_curve to check if the model is overfitting or underfitting\n\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, valid_scores = learning_curve(LinearRegression(), X_train, y_train, train_sizes=np.linspace(0.001, 1.0, 40), cv=5, scoring=\"neg_root_mean_squared_error\") # learning curve for different test sizes\n\ntrain_errors = -train_scores.mean(axis=1)\nvalid_errors = -valid_scores.mean(axis=1)\n\nplt.figure(figsize=(6, 4)) # plot the learning curve\nplt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\nplt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n\nplt.xlabel(\"Training set size\")\nplt.ylabel(\"RMSE\")\nplt.grid()\nplt.legend(loc=\"upper right\")\nplt.show()\n\n\n\n\nThe learning curve plot shows a classic case of the model underfitting the data. When the training set data size is low, the data can be described fairly accurately by a linear regression model. Therefore, the RMSE is low for lower training set size. However, as the training set data size increases, the value of RMSE steadily increases until it reaches a constant value. At the point, adding or removing the training data does not make the model better or worse. Similarly, for the validation sets, if the size of the data set is low, the validation error is very high. However, as more data is added, the validation drops down until it reaches a constant value.\nSince the model is underfitting, we will implement a higher degree polynomial regressor. We will import PolynomialFeatures from scikit-learn.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_features = PolynomialFeatures(degree=2,include_bias=False)\nX_poly_train = poly_features.fit_transform(X_train) # transform feature labels into the polynomial form\nlin_reg.fit(X_poly_train,y_train) # fit linear regression on the polynomially transformed variables\n\n-cross_val_score(lin_reg,X_poly_train,y_train,cv=5,scoring='neg_root_mean_squared_error') # the cross validation score\n\narray([119.09738289, 121.77779834, 121.89287345, 124.76923189,\n       120.9170965 ])\n\n\nWe can see that the polynomial regressor has performed marginally better than the linear regressor with an average cross val score of 120 across the 5 folds. Let us plot the learning curve to see whether the model is overfitting or underfitting.\n\n\n\n\n\nAs we can see from the learning curve, the regression model is still underfitting. Therefore, let us use nonlinear regressors instead. We will use DecisionTreeRegressor first. We will use RandomizedSearchCV to tune the hyperparameters.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\ndecision_reg = DecisionTreeRegressor(random_state=42)\nparam_random = [{'max_depth':randint(low=2,high=20)},{'min_samples_leaf':randint(low=2,high=20)}] # hyperparameter tuning\nrand_dec_reg = RandomizedSearchCV(decision_reg,param_distributions=param_random,scoring='neg_root_mean_squared_error',n_iter=10,random_state=42) \nrand_dec_reg.fit(X_train,y_train) # training the decision tree regressor\n\n-cross_val_score(rand_dec_reg,X_train,y_train,cv=5,scoring='neg_root_mean_squared_error')\n\narray([57.64518755, 59.89346893, 58.21764267, 56.4518072 , 61.12587745])\n\n\nThe cross validation score for this model is close to 60 on average across the five validation folds. This score is much better than the linear and polynomial regressors. Let us look at the learning curve for this model.\n\n\n\n\n\nThe validation error decreases as the training set size increases but the validation error never approaches the training error. Thus, the model seems to be slightly overfitting. Let us now implement the RandomForestRegressor.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrand_reg = RandomForestRegressor(random_state=42)\nparam_random = [{'n_estimators':randint(low=20,high=100)}] # hyperparameter tuning\nrandom_rand_reg = RandomizedSearchCV(rand_reg,param_distributions=param_random,scoring='neg_root_mean_squared_error',n_iter=10,random_state=42) \nrandom_rand_reg.fit(X_train,y_train) # training the decision tree regressor\n\n-cross_val_score(random_rand_reg,X_train,y_train,cv=3,scoring='neg_root_mean_squared_error')\n\narray([51.27749959, 48.82450026, 51.86218269])\n\n\nThe RandomForestRegressor produces a cross validation score of 50 on average across the three validation folds. Thus, the RandomForestRegressor performs the best among all the models tested here. Let us look at the root mean log squared error (RMLSE) for the test set using this model.\n\nfrom sklearn.metrics import mean_squared_log_error\n\ny_pred = random_rand_reg.predict(X_test) # predictions on the test set\nnp.sqrt(mean_squared_log_error(y_pred,y_test))\n\n0.3858129750838661\n\n\nThe RMLSE of the RandomForestRegressor is 0.386."
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html#feature-importances",
    "href": "posts/Linear and Nonlinear Regression/index.html#feature-importances",
    "title": "Linear and Nonlinear Regression",
    "section": "Feature Importances",
    "text": "Feature Importances\nLet us look at how the RandomForestRegressor weighs the features in predictions. As shown in the bar plot below, the hour of the day is the most important feature for the Random Forest model.\n\nimportances = random_rand_reg.best_estimator_.feature_importances_\nplt.title('Feature Importances for the Random Forest Model')\nplt.barh(range(len(importances)), importances, color='g', align='center')\nplt.yticks(range(len(importances)), X_train.columns)\nplt.xlabel('Relative Weights')\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html",
    "href": "posts/Probability theory and random variables/index.html",
    "title": "Probability Theory and Random Variables - Logistic Regression",
    "section": "",
    "text": "In this blog, we will explore the probability theory underlying Logistic Regression. Unlike other Normal and Poisson Regression models, Logistic Regression is often used when the target labels are categorical. Thus, Logistic Regression is most suitable for classification problems. For example, let us imagine that we want to predict whether a patient is likely to have a heart attack or not. Therefore, the categorical model value Y :\n\\[\\begin{equation}\n  Y =\n  \\begin{cases}\n    0,  no \\hspace{0.1cm} heart \\hspace{0.1cm} attack \\\\\n    1,  heart \\hspace{0.1cm} attack\n  \\end{cases}\n\\end{equation}\\]\nLet us assume that the target Y is dependent on features \\(X_1\\), \\(X_2\\), and \\(X_3\\) which represent the patient age, patient weight, and cholesterol level of the patient."
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html#introduction",
    "href": "posts/Probability theory and random variables/index.html#introduction",
    "title": "Probability Theory and Random Variables - Logistic Regression",
    "section": "",
    "text": "In this blog, we will explore the probability theory underlying Logistic Regression. Unlike other Normal and Poisson Regression models, Logistic Regression is often used when the target labels are categorical. Thus, Logistic Regression is most suitable for classification problems. For example, let us imagine that we want to predict whether a patient is likely to have a heart attack or not. Therefore, the categorical model value Y :\n\\[\\begin{equation}\n  Y =\n  \\begin{cases}\n    0,  no \\hspace{0.1cm} heart \\hspace{0.1cm} attack \\\\\n    1,  heart \\hspace{0.1cm} attack\n  \\end{cases}\n\\end{equation}\\]\nLet us assume that the target Y is dependent on features \\(X_1\\), \\(X_2\\), and \\(X_3\\) which represent the patient age, patient weight, and cholesterol level of the patient."
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html#data-distribution-model",
    "href": "posts/Probability theory and random variables/index.html#data-distribution-model",
    "title": "Probability Theory and Random Variables - Logistic Regression",
    "section": "Data Distribution Model",
    "text": "Data Distribution Model\nIn our case, the target variable Y is binary, i.e., it can either take 0 or 1. Thus, we utilize the Bernoulli probability distribution model. The probability of patient i having a heart attack is denoted by pi\n\\[\nY_{i}|p_{i} \\sim Bernoulli(p_{i})\n\\tag{1}\\]\nwhere the expected or the mean value is given as\n\\[\nE(Y_{i}|p_{i}) = p_{i}\n\\tag{2}\\]\nThus, the expected value of the target variable Y is the probability of Y itself. Furthermore, this probability follows the Bernoulli distribution. Now, the final piece is to describe how the probability of patient i is related to the attributes \\(X_{1i}\\), \\(X_{2i}\\), and \\(X_{3i}\\). For simplicity, let us assume, for now, that the probability of heart attack for patient i is only dependent on the feature \\(X_{1i}\\). Let us also define a function \\(F(p_{i})\\), which produces a linear relationship between the probability and the feature as shown below.\n\\[\nF(p_{i}) = \\alpha + \\beta X_{1i}\n\\tag{3}\\]\nThe right hand side of Equation 3 is a linear function that can span the entire real space. Therefore, the left hand side also has to span the entire real space. Here’s where we introduce the logit function. The logit function is defined as the logarithm of the odds of the probability distribution. Thus, the relationship becomes:\n\\[\nF(p_{i}) = \\log{}(\\frac{p_{i}}{1 - p_{i}}) = \\alpha + \\beta X_{1i}\n\\tag{4}\\]\nWhich on solving further, gives\n\\[\nodds_{i} = \\frac{p_{i}}{1-p_{i}} = e^{\\alpha + \\beta X_{1i}}\n\\tag{5}\\]\nSolving further,\n\\[\np_{i} = \\frac{e^{\\alpha + \\beta X_{1i}}}{1 + e^{\\alpha + \\beta X_{1i}}}\n\\tag{6}\\]\nLet us assume that the value of \\(\\alpha\\) is - 4 and the value of \\(\\beta\\) is 0.1 for our case. Using these values for our heart attack analysis, we can generate the following plots.\n\n\n\n\n\nThe figures show that even though the odds and the probability vary non-linearly with respect to the patient age, the logarithm of odds varies linearly with the patient age. In general, for multiple features \\(X_{1}\\), \\(X_{2}\\), …, \\(X_{n}\\), we can define the logarithm of odds to be:\n\\[\n\\log{}(\\frac{p_{i}}{1 - p_{i}}) = \\alpha + \\beta X_{1} + \\beta_{2} X_2 + ... + \\beta_{n} X_{n}\n\\tag{7}\\]\nEach new feature added has a multiplicative effect on the odds of heart attack, i.e., adding feature \\(X_{2}\\) will result in multiplication by \\(e^{\\beta_{2}X_{2}}\\). The coefficients \\(\\alpha\\), \\(\\beta\\), …, \\(\\beta_{n}\\) are estimated through a combination of our “prior” knowledge and “posterior” simulations on known datasets."
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html#priors-and-posteriors",
    "href": "posts/Probability theory and random variables/index.html#priors-and-posteriors",
    "title": "Probability Theory and Random Variables - Logistic Regression",
    "section": "Priors and Posteriors",
    "text": "Priors and Posteriors"
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html#section",
    "href": "posts/Probability theory and random variables/index.html#section",
    "title": "Probability Theory and Random Variables - Logistic Regression",
    "section": "",
    "text": "from sklearn import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\niris = datasets.load_iris()\niris_data = iris.data # get data values\niris_target = iris.target # target labels\n\n# Display iris target labels\n\nprint(iris_target[iris_target == 0].shape) # print the number of setosa target datapoints\nprint(iris_target[iris_target == 1].shape) # print the number of versi-color target datapoints\n\n(50,)\n(50,)\n\n\nTherefore, the number of setosa and versi-color target datapoints is 50 each. Let us now segregate the iris dataset such that we select only setosa and versi-color target variables. Furthermore, let us look at how the sepal length varies across target variables.\n\niris_data = iris_data[0:100] # filter only setosa and versi-color target data\niris_target = iris_target[0:100] # filter only setosa and versi-color target labels\nplt.scatter(iris_data[:,0], iris_target)\nplt.xlabel('Sepal length (cm)')\nplt.ylabel('Target name')\nplt.grid()\n\n\n\n\nSince the target name is categorical and binary (either 0 or 1), we can see that the data is separated. Let us now import the LogisticRegression from scikit-learn. Logistic regression works by fitting a logit function to the separated data. If the value corresponding to a given sepal length falls below 0.5, then the model predicts the class as 0 (i.e. setosa). However, when the value corresponding to a given sepal length is greater than 0.5, then the model predicts the class as 1 (i.e., versi-color)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.special import expit\n\nmodel = LogisticRegression(random_state=42)\nmodel.fit(iris_data[:,0].reshape(-1,1),iris_target)\n\n# Test the logistic regression model with a test sample\n\nsepal_sample = np.linspace(4.3,7,100) # a random sample of sepal lengths\ntarget_sample = sepal_sample.reshape(-1,1)*model.coef_ + model.intercept_\nlogistic_fit = expit(target_sample)\n\n# Plotting\nplt.scatter(iris_data[:,0],iris_target, c=iris_target,label = \"Sepal length\")\nplt.plot(sepal_sample,logistic_fit.ravel(),c='blue',label='Logistic regression')\nplt.axhline(.5, color=\"red\", label=\"Decision threshold\")\nplt.legend(loc='lower right')\nplt.grid()"
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html#classification-example",
    "href": "posts/Probability theory and random variables/index.html#classification-example",
    "title": "Probability Theory and Random Variables - Logistic Regression",
    "section": "Classification Example",
    "text": "Classification Example\nIn this section, an example classification problem is solved using Logistic Regression. In this example, we will import the iris dataset. This is a classic dataset which is used in classification problems. The dataset contains three class, class ‘0’ represents setosa, class ‘1’ represents versicolor, and class ‘2’ represents virginica. These classes depend on the features, sepal length, sepal width, petal length, and petal width.\nTo implement logistic regression on this dataset, let us first import the necessary libraries\n\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\niris = datasets.load_iris()\niris_data = iris.data # get data values\niris_target = iris.target # target labels\n\n# Display iris target labels\n\nprint(iris_target[iris_target == 0].shape) # print the number of setosa target datapoints\nprint(iris_target[iris_target == 1].shape) # print the number of versi-color target datapoints\n\n(50,)\n(50,)\n\n\nTherefore, the number of setosa and versi-color target datapoints is 50 each. Let us now segregate the iris dataset such that we select only setosa and versi-color target variables. Furthermore, let us look at how the sepal length varies across target variables.\n\niris_data = iris_data[0:100] # filter only setosa and versi-color target data\niris_target = iris_target[0:100] # filter only setosa and versi-color target labels\nplt.scatter(iris_data[:,0], iris_target)\nplt.xlabel('Sepal length (cm)')\nplt.ylabel('Target name')\nplt.grid()\n\n\n\n\nSince the target name is categorical and binary (either 0 or 1), we can see that the data is separated. Let us now import the LogisticRegression from scikit-learn. Logistic regression works by fitting a logit function to the separated data. If the value corresponding to a given sepal length falls below 0.5, then the model predicts the class as 0 (i.e. setosa). However, when the value corresponding to a given sepal length is greater than 0.5, then the model predicts the class as 1 (i.e., versi-color)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.special import expit\n\nmodel = LogisticRegression(random_state=42)\nmodel.fit(iris_data[:,0].reshape(-1,1),iris_target)\n\n# Test the logistic regression model with a test sample\n\nsepal_sample = np.linspace(4.3,7,100) # a random sample of sepal lengths\ntarget_sample = sepal_sample.reshape(-1,1)*model.coef_ + model.intercept_\nlogistic_fit = expit(target_sample)\n\n# Plotting\nplt.scatter(iris_data[:,0],iris_target, c=iris_target,label = \"Sepal length\")\nplt.plot(sepal_sample,logistic_fit.ravel(),c='blue',label='Logistic regression')\nplt.axhline(.5, color=\"red\", label=\"Decision threshold\")\nplt.legend(loc='lower right')\nplt.grid()"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering of Customer Personality Analysis Data",
    "section": "",
    "text": "The Customer Personality Analysis dataset contains various different features to aid a company in customer segmentation. Customer segmentation is used to cluster customers into different categories based on their demographic, lifestyle, behavior, and so on. The dataset, which is hosted on Kaggle, has the following features:\n\nYear_Birth: Customer’s year of birth.\nEducation: Customer’s education level.\nMarital_Status: Customer’s marital status.\nIncome: Customer’s yearly household income.\nKidhome: Number of kids at the customer’s home.\nTeenhome: Number of teenagers at the customer’s home.\nDt_Customer: Date on which the customer enrolled with the company.\nRecency: Number of days since the customer’s last purchase.\nMntWines: Amount of money spent on wines in the last two years.\nMntFruits: Amount of money spent on fruits in the last two years.\nMntMeatProducts: Amount of money spent on meat in the last two years.\nMntFishProducts: Amount of money spent on fish in the last two years.\nMntSweetProducts: Amount of money spent on sweets in the last two years.\nMntGoldProds: Amount of money spent on gold in the last two years.\nNumDealsPurchases: Number of purchases made with a discount\nAcceptedCmp1: 1 if the customer accepted the offer in the 1st campaign, 0 otherwise\nAcceptedCmp2: 1 if the customer accepted the offer in the 2nd campaign, 0 otherwise\nAcceptedCmp3: 1 if the customer accepted the offer in the 3rd campaign, 0 otherwise\nAcceptedCmp4: 1 if the customer accepted the offer in the 4th campaign, 0 otherwise\nAcceptedCmp5: 1 if the customer accepted the offer in the 5th campaign, 0 otherwise\nResponse: 1 if the customer accepted the offer in the last campaign, 0 otherwise\n\nGiven these features, we want to perform clustering to split the data into customer segments."
  },
  {
    "objectID": "posts/Clustering/index.html#introduction",
    "href": "posts/Clustering/index.html#introduction",
    "title": "Clustering of Customer Personality Analysis Data",
    "section": "",
    "text": "The Customer Personality Analysis dataset contains various different features to aid a company in customer segmentation. Customer segmentation is used to cluster customers into different categories based on their demographic, lifestyle, behavior, and so on. The dataset, which is hosted on Kaggle, has the following features:\n\nYear_Birth: Customer’s year of birth.\nEducation: Customer’s education level.\nMarital_Status: Customer’s marital status.\nIncome: Customer’s yearly household income.\nKidhome: Number of kids at the customer’s home.\nTeenhome: Number of teenagers at the customer’s home.\nDt_Customer: Date on which the customer enrolled with the company.\nRecency: Number of days since the customer’s last purchase.\nMntWines: Amount of money spent on wines in the last two years.\nMntFruits: Amount of money spent on fruits in the last two years.\nMntMeatProducts: Amount of money spent on meat in the last two years.\nMntFishProducts: Amount of money spent on fish in the last two years.\nMntSweetProducts: Amount of money spent on sweets in the last two years.\nMntGoldProds: Amount of money spent on gold in the last two years.\nNumDealsPurchases: Number of purchases made with a discount\nAcceptedCmp1: 1 if the customer accepted the offer in the 1st campaign, 0 otherwise\nAcceptedCmp2: 1 if the customer accepted the offer in the 2nd campaign, 0 otherwise\nAcceptedCmp3: 1 if the customer accepted the offer in the 3rd campaign, 0 otherwise\nAcceptedCmp4: 1 if the customer accepted the offer in the 4th campaign, 0 otherwise\nAcceptedCmp5: 1 if the customer accepted the offer in the 5th campaign, 0 otherwise\nResponse: 1 if the customer accepted the offer in the last campaign, 0 otherwise\n\nGiven these features, we want to perform clustering to split the data into customer segments."
  },
  {
    "objectID": "posts/Clustering/index.html#importing-the-dataset",
    "href": "posts/Clustering/index.html#importing-the-dataset",
    "title": "Clustering of Customer Personality Analysis Data",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nFirst, we import the libraries required to perform the initial data analysis. Next, let us store the dataset in a new variable, customer_data.\n\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis/download?datasetVersionNumber=1' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as customer_file:\n    customer_file.extractall(path='datasets')\n    \ncustomer_data = pd.read_csv(Path('datasets/marketing_campaign.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "posts/Clustering/index.html#analyzing-the-data",
    "href": "posts/Clustering/index.html#analyzing-the-data",
    "title": "Clustering of Customer Personality Analysis Data",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\nLet us look at the dataset by using the head() method.\n\ncustomer_data.head()\n\n\n\n\n\n\n\n\nID\nYear_Birth\nEducation\nMarital_Status\nIncome\nKidhome\nTeenhome\nDt_Customer\nRecency\nMntWines\n...\nNumWebVisitsMonth\nAcceptedCmp3\nAcceptedCmp4\nAcceptedCmp5\nAcceptedCmp1\nAcceptedCmp2\nComplain\nZ_CostContact\nZ_Revenue\nResponse\n\n\n\n\n0\n5524\n1957\nGraduation\nSingle\n58138.0\n0\n0\n04-09-2012\n58\n635\n...\n7\n0\n0\n0\n0\n0\n0\n3\n11\n1\n\n\n1\n2174\n1954\nGraduation\nSingle\n46344.0\n1\n1\n08-03-2014\n38\n11\n...\n5\n0\n0\n0\n0\n0\n0\n3\n11\n0\n\n\n2\n4141\n1965\nGraduation\nTogether\n71613.0\n0\n0\n21-08-2013\n26\n426\n...\n4\n0\n0\n0\n0\n0\n0\n3\n11\n0\n\n\n3\n6182\n1984\nGraduation\nTogether\n26646.0\n1\n0\n10-02-2014\n26\n11\n...\n6\n0\n0\n0\n0\n0\n0\n3\n11\n0\n\n\n4\n5324\n1981\nPhD\nMarried\n58293.0\n1\n0\n19-01-2014\n94\n173\n...\n5\n0\n0\n0\n0\n0\n0\n3\n11\n0\n\n\n\n\n5 rows × 29 columns\n\n\n\nLet’s look at the non-null entries in the dataset using the info() method below.\n\ncustomer_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2240 entries, 0 to 2239\nData columns (total 29 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   ID                   2240 non-null   int64  \n 1   Year_Birth           2240 non-null   int64  \n 2   Education            2240 non-null   object \n 3   Marital_Status       2240 non-null   object \n 4   Income               2216 non-null   float64\n 5   Kidhome              2240 non-null   int64  \n 6   Teenhome             2240 non-null   int64  \n 7   Dt_Customer          2240 non-null   object \n 8   Recency              2240 non-null   int64  \n 9   MntWines             2240 non-null   int64  \n 10  MntFruits            2240 non-null   int64  \n 11  MntMeatProducts      2240 non-null   int64  \n 12  MntFishProducts      2240 non-null   int64  \n 13  MntSweetProducts     2240 non-null   int64  \n 14  MntGoldProds         2240 non-null   int64  \n 15  NumDealsPurchases    2240 non-null   int64  \n 16  NumWebPurchases      2240 non-null   int64  \n 17  NumCatalogPurchases  2240 non-null   int64  \n 18  NumStorePurchases    2240 non-null   int64  \n 19  NumWebVisitsMonth    2240 non-null   int64  \n 20  AcceptedCmp3         2240 non-null   int64  \n 21  AcceptedCmp4         2240 non-null   int64  \n 22  AcceptedCmp5         2240 non-null   int64  \n 23  AcceptedCmp1         2240 non-null   int64  \n 24  AcceptedCmp2         2240 non-null   int64  \n 25  Complain             2240 non-null   int64  \n 26  Z_CostContact        2240 non-null   int64  \n 27  Z_Revenue            2240 non-null   int64  \n 28  Response             2240 non-null   int64  \ndtypes: float64(1), int64(25), object(3)\nmemory usage: 507.6+ KB\n\n\nAs we can see in the dataset, the Income column has a few null values. Let us use the SimpleImputer class from scikit-learn to replace the null values with the column median.\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')\n\ncustomer_data['Income'] = imputer.fit_transform(customer_data[['Income']]) # Replace the 'Income' column null values with median\n\nNow, let us look at the customer data using the info() method.\n\ncustomer_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2240 entries, 0 to 2239\nData columns (total 29 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   ID                   2240 non-null   int64  \n 1   Year_Birth           2240 non-null   int64  \n 2   Education            2240 non-null   object \n 3   Marital_Status       2240 non-null   object \n 4   Income               2240 non-null   float64\n 5   Kidhome              2240 non-null   int64  \n 6   Teenhome             2240 non-null   int64  \n 7   Dt_Customer          2240 non-null   object \n 8   Recency              2240 non-null   int64  \n 9   MntWines             2240 non-null   int64  \n 10  MntFruits            2240 non-null   int64  \n 11  MntMeatProducts      2240 non-null   int64  \n 12  MntFishProducts      2240 non-null   int64  \n 13  MntSweetProducts     2240 non-null   int64  \n 14  MntGoldProds         2240 non-null   int64  \n 15  NumDealsPurchases    2240 non-null   int64  \n 16  NumWebPurchases      2240 non-null   int64  \n 17  NumCatalogPurchases  2240 non-null   int64  \n 18  NumStorePurchases    2240 non-null   int64  \n 19  NumWebVisitsMonth    2240 non-null   int64  \n 20  AcceptedCmp3         2240 non-null   int64  \n 21  AcceptedCmp4         2240 non-null   int64  \n 22  AcceptedCmp5         2240 non-null   int64  \n 23  AcceptedCmp1         2240 non-null   int64  \n 24  AcceptedCmp2         2240 non-null   int64  \n 25  Complain             2240 non-null   int64  \n 26  Z_CostContact        2240 non-null   int64  \n 27  Z_Revenue            2240 non-null   int64  \n 28  Response             2240 non-null   int64  \ndtypes: float64(1), int64(25), object(3)\nmemory usage: 507.6+ KB\n\n\nWe can now see that there are no null values in the dataset. Let us now look at the ‘date’ variables. Instead of using Year_Birth, perhaps it is better to use the customer age. Furthermore, let us convert the Dt_Customer feature to the ‘datetime’ format and create a Num_Yrs_Customer column which represents the number of years that the person has been a customer.\n\nfrom datetime import date\nfrom datetime import datetime\n\ncustomer_data['Age'] = 2023 - customer_data['Year_Birth'] # Customer Age\ncustomer_data.drop('Year_Birth',axis=1,inplace=True) # Drop the Birth Year from the dataset\n\ncustomer_data['Dt_Customer'] = pd.to_datetime(customer_data['Dt_Customer'], format='%d-%m-%Y')\ncustomer_data['Num_Yrs_Customer'] = pd.Timestamp('now').year - customer_data['Dt_Customer'].dt.year # number of years that the person has been a customer\ncustomer_data.drop('Dt_Customer',axis=1,inplace=True) # Drop the Dt_Customer column\n\nNow, let us do some more feature engineering by creating new columns. We observe that there are many different expenses such as MntWines , MntFruits , and so on. We will now add another column called Total_Spending which sums up the total expenditure for the past two years. We will create another column called Num_Accept_Cmps which adds up all the campaigns that have been accepted by the customer.\n\ncustomer_data['Total_Spending'] = customer_data['MntFishProducts'] + customer_data['MntFruits'] + customer_data['MntGoldProds'] + customer_data['MntMeatProducts'] + customer_data['MntSweetProducts'] + customer_data['MntWines'] # create Total_Spending column\n\ncustomer_data['Num_Accept_Cmps'] = customer_data['AcceptedCmp1'] + customer_data['AcceptedCmp2'] + customer_data['AcceptedCmp3'] + customer_data['AcceptedCmp4'] + customer_data['AcceptedCmp5'] + customer_data['Response'] # create the Num_Accept_Cmps column\n\nThe Marital_Status feature has many categories such as Married, Together, Single, Divorced, Widow, Alone, Absurd, and YOLO.\n\ncustomer_data.Marital_Status.value_counts()\n\nMarital_Status\nMarried     864\nTogether    580\nSingle      480\nDivorced    232\nWidow        77\nAlone         3\nAbsurd        2\nYOLO          2\nName: count, dtype: int64\n\n\nIt would be beneficial if we could reduce the number of categories by clubbing some of them together. For example, Alone, Absurd, Widow, Divorced, and YOLO would fall under the category of ‘Alone’ (which can be represented by 0). The rest of the columns can be replaced by 1. Furthermore, let us add a column called Parent whose value is 0 if there are no kids or teenagers. We will also add a column called Family_members which accounts for the total number of members in the household. Lastly, we will drop all the redundant columns from the dataset.\n\n# Replace redundant categories in marital status\n\ncustomer_data['Marital_Status'] = customer_data[\"Marital_Status\"].replace({\"Married\":1, \"Together\":1, \"Absurd\":0, \"Widow\":0, \"YOLO\":0, \"Divorced\":0, \"Single\":0,\"Alone\":0})\n\n# Add parent feature\n\ncustomer_data['Parent'] = np.where(customer_data.Kidhome + customer_data.Teenhome &gt; 0, 1, 0)\n\n# Add Family members feature\n\ncustomer_data['Family_members'] = customer_data.Marital_Status.replace({0:1,1:2}) + customer_data.Kidhome + customer_data.Teenhome\n\n# Drop remaining unnecessary columns\n\ncustomer_data.drop(['Z_CostContact','Z_Revenue','ID'],axis = 1,inplace=True)\n\nLet us look at the histograms of some of the features.\n\n# plotting some of the features\n\nto_plot = ['Income','Recency','Age','Total_Spending'] \n\ncustomer_features = customer_data[to_plot]\ncustomer_features.hist(bins=50, figsize=(12, 12))\nplt.show()\n\n\n\n\nFrom the histograms, we can observe that both Income and Age features have outliers that need to be removed.\n\n# Remove outliers from Income and Age\n\ncustomer_data = customer_data[(customer_data['Age'] &lt; 100)] # Restrict Age values to below 100\ncustomer_data = customer_data[(customer_data['Income'] &lt; 150000) ] # Restrict Income values to below 150000\n\nNow, let us plot the histograms. As we can see, all the outliers have been removed.\n\n\n\n\n\nNow, let us plot the correlation matrix for the dataset.\n\ncustomer_corr = customer_data.drop('Education',axis = 1) # Drop non-integer data\nfig, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(customer_corr.corr(), annot=True, cmap='YlGnBu', center=0, ax=ax)\nplt.title('Linear Correlations between attributes')\nplt.show()\n\n\n\n\nAs we can see from the correlation matrix, there are multiple features which are highly correlated with other features. Therefore, we will do dimensionality reduction by using Principal Component Analysis.\nBut first, let us convert the remaining categorical variable Education to numerical values. To do this, let us use LabelEncoder.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlencoder = LabelEncoder() # label encoder\n\ncustomer_data['Education'] = lencoder.fit_transform(customer_data['Education'])\n\nLet us standardize the features using StandardScaler.\n\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\n\n# Segregate features which are supposed to be scaled\n\nto_scale = ['Income','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n'NumStorePurchases', 'NumWebVisitsMonth','Age', 'Num_Yrs_Customer', 'Total_Spending',\n'Num_Accept_Cmps', 'Parent', 'Family_members']\n\ncustomer_data[to_scale] = std_scaler.fit_transform(customer_data[to_scale]) # Scale the columns\n\nLet us plot the histograms of the same representative features again.\n\n\n\n\n\nWe can see that while the features Income and Age have gaussian like distributions and the recency feature is almost a perfect rectangle, the Total_Spending feature has a very long tail. Let us use the boxcox scaler to transform this feature.\n\nfrom scipy.stats import boxcox\n\ncustomer_data['Total_Spending'] = boxcox(customer_data['Total_Spending'],0) # Take the logarithm of the feature\n\n# plot histograms again\n\nto_plot = ['Income','Recency','Age','Total_Spending'] \n\ncustomer_features = customer_data[to_plot]\ncustomer_features.hist(bins=50, figsize=(12, 12))\nplt.show()\n\n\n\n\nLet us remove the outliers from this Total_Spending feature.\n\n# Remove outliers from Total_Spending\n\ncustomer_data = customer_data[(customer_data['Total_Spending'] &gt; -6)] # Restrict Total_Spending values to above -6\n\nAs we can see now, the Total_Spending feature has a better-looking distribution."
  },
  {
    "objectID": "posts/Clustering/index.html#dimensionality-reduction",
    "href": "posts/Clustering/index.html#dimensionality-reduction",
    "title": "Clustering of Customer Personality Analysis Data",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nSince the dataset has many features, we choose to reduce the dimensionality of our dataset using Principal Component Analysis (PCA). We will import the PCA class from scikit-learn. The number of components will be determined using RandomizedSearchCV for each classifier.\n\n# Import the PCA class\n\nfrom sklearn.decomposition import PCA"
  },
  {
    "objectID": "posts/Clustering/index.html#clustering",
    "href": "posts/Clustering/index.html#clustering",
    "title": "Clustering of Customer Personality Analysis Data",
    "section": "Clustering",
    "text": "Clustering\nLet us first import the KMeans algorithm from scikit-learn. Next, let us use RandomizedSearchCV to find the best estimator with the optimal number of PCA components and KMeans clusters.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\n\nkmeans_clf = make_pipeline(PCA(random_state=42),KMeans(random_state=42,n_init=10)) # Import pipeline for dimensionality reduction and clustering\nparam_distrib = {\"pca__n_components\":np.arange(3,20),\"kmeans__n_clusters\":np.arange(2,10)} # random parameter distribution\n\nrnd_search = RandomizedSearchCV(kmeans_clf, param_distrib, n_iter=10, cv=3,random_state=42)\n\nrnd_search.fit(customer_data)\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('pca', PCA(random_state=42)),\n                                             ('kmeans',\n                                              KMeans(n_init=10,\n                                                     random_state=42))]),\n                   param_distributions={'kmeans__n_clusters': array([2, 3, 4, 5, 6, 7, 8, 9]),\n                                        'pca__n_components': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])},\n                   random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('pca', PCA(random_state=42)),\n                                             ('kmeans',\n                                              KMeans(n_init=10,\n                                                     random_state=42))]),\n                   param_distributions={'kmeans__n_clusters': array([2, 3, 4, 5, 6, 7, 8, 9]),\n                                        'pca__n_components': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])},\n                   random_state=42)estimator: PipelinePipeline(steps=[('pca', PCA(random_state=42)),\n                ('kmeans', KMeans(n_init=10, random_state=42))])PCAPCA(random_state=42)KMeansKMeans(n_init=10, random_state=42)\n\n\nWe can now see what the best estimator is. As we can see, the optimal number of PCA components is 5 and optimal number of KMeans clusters is 3.\n\nrnd_search.best_estimator_\n\nPipeline(steps=[('pca', PCA(n_components=5, random_state=42)),\n                ('kmeans', KMeans(n_clusters=3, n_init=10, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('pca', PCA(n_components=5, random_state=42)),\n                ('kmeans', KMeans(n_clusters=3, n_init=10, random_state=42))])PCAPCA(n_components=5, random_state=42)KMeansKMeans(n_clusters=3, n_init=10, random_state=42)\n\n\nTherefore, we will use these parameters henceforth.\n\nkm = KMeans(n_clusters=3, random_state=42,n_init=10) # Import KMeans clustering\npca = PCA(n_components=5, random_state=42) # Import PCA\n\ncustomer_data_reduced = pca.fit_transform(customer_data) # reduce pca components\nkm.fit(customer_data_reduced)\n\nKMeans(n_clusters=3, n_init=10, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, n_init=10, random_state=42)\n\n\nLet us look at the silhouette_score for the KMeans cluster.\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(customer_data_reduced,km.labels_)\n\n0.26082278755573535\n\n\nThe silhouette score for this clustering algorithm is around 0.26. Let us plot the silhouette diagram to investigate further.\n\n# Plotting silhouette coefficients\n\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(customer_data_reduced) for k in range(1, 10)] # kmeans cluster\nsilhouette_scores = [silhouette_score(customer_data_reduced, model.labels_) for model in kmeans_per_k[1:]]\n\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(11, 9))\n\nfor k in (3, 4, 5, 6):\n    plt.subplot(2, 2, k - 2)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(customer_data_reduced, y_pred)\n\n    padding = len(customer_data_reduced) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (3, 5):\n        plt.ylabel(\"Cluster\")\n    \n    if k in (5, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"Silhouette Coefficient\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if k in (3, 4):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n  \n    plt.axvline(x=silhouette_scores[k], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\n    \nplt.show()\n\n\n\n\nThe knife edge plot shows that although k = 3 is the optimum number of clusters by performing randomized search, all the remaining clusters produce similar silhouette scores. Let’s take a look at the elbow plot below, with the elbow shown at k = 3. As we can see from the plot, the inertia drops slowly if we increase k above 3.\n\ninertias = [model.inertia_ for model in kmeans_per_k]\n\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\n\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.axvline(x=4, color=\"red\", linestyle=\"--\")\nplt.grid()\nplt.show()\n\n\n\n\nNow, let us compare the performance of KMeans cluster with an Agglomerative cluster. The code is shown below.\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n\nagc = AgglomerativeClustering(n_clusters=3)\nagc.fit(customer_data_reduced)\n\nsilhouette_score(customer_data_reduced,agc.labels_)\n\n0.23970062590927582\n\n\nThe Agglomerative cluster does slightly worse than the KMeans cluster. Let us use the KMeans cluster to analyze the results."
  },
  {
    "objectID": "posts/Clustering/index.html#analyzing-results",
    "href": "posts/Clustering/index.html#analyzing-results",
    "title": "Clustering of Customer Personality Analysis Data",
    "section": "Analyzing Results",
    "text": "Analyzing Results\nLet us plot a bar chart of the 4 clusters as evaluated by KMeans.\n\ny_pred = km.predict(customer_data_reduced) # Cluster predictions\ncustomer_data['cluster'] = y_pred # Assigning a new cluster column\n\n\nbar_plot = sns.countplot(x=customer_data[\"cluster\"])\nbar_plot.set_title('Cluster distribution')\nplt.show()\n\n\n\n\nThe bar plot shows that most of the data is being clustered into ‘Cluster 0’ and the least amount of data is allocated to ‘Cluster 2’. Otherwise, the data seems to be well distributed. Let us also look at a scatterplot to look at the distribution of cluster.\n\n# Cluster customers based on income and expenditure\n\n# Use inverse transform to restore original data\ncustomer_data['Total_Spending'] = np.exp(customer_data['Total_Spending']) # Inverse of boxcox\ncustomer_data[to_scale] = std_scaler.inverse_transform(customer_data[to_scale]) # inverse transform to restore initial variables\nscatter = sns.scatterplot(data=customer_data,x=customer_data['Total_Spending'],y=customer_data['Income'],hue=customer_data['cluster'])\n\nscatter.set_title('Customer clusters based on Income and Expenditure')\nplt.legend()\nplt.show()\n\n\n\n\nFrom the scatterplot, we can make the following conclusions:\n\nCluster ‘0’ is on average, comprised of customers with intermediate income and intermediate total expenditure.\nCluster ‘1’ is on average, comprised of customers with low income and low total expenditure.\nCluster ‘2’ is on average, comprised of customers with high income and high total expenditure."
  },
  {
    "objectID": "posts/Anomaly detection/index.html",
    "href": "posts/Anomaly detection/index.html",
    "title": "Anomaly Detection: Credit Card Fraud Analysis",
    "section": "",
    "text": "The credit card fraud detection dataset comprises of all the transactions made by credit card holders in the European Union in September 2013. Out of the total 284,807 transactions that have taken place, only 492 transactions are fraudulent. The dataset contains the following features: Time, which indicates the number of transactions that elapsed between the current transaction and the first transaction in the dataset, V1, …, V28 are all anonymous features obtained through principal component analysis. Amount indicates the transaction amount and Class is the outcome which set to 1 in case of fraud and 0 in case there is no fraud."
  },
  {
    "objectID": "posts/Anomaly detection/index.html#introduction",
    "href": "posts/Anomaly detection/index.html#introduction",
    "title": "Anomaly Detection: Credit Card Fraud Analysis",
    "section": "",
    "text": "The credit card fraud detection dataset comprises of all the transactions made by credit card holders in the European Union in September 2013. Out of the total 284,807 transactions that have taken place, only 492 transactions are fraudulent. The dataset contains the following features: Time, which indicates the number of transactions that elapsed between the current transaction and the first transaction in the dataset, V1, …, V28 are all anonymous features obtained through principal component analysis. Amount indicates the transaction amount and Class is the outcome which set to 1 in case of fraud and 0 in case there is no fraud."
  },
  {
    "objectID": "posts/Anomaly detection/index.html#importing-the-dataset",
    "href": "posts/Anomaly detection/index.html#importing-the-dataset",
    "title": "Anomaly Detection: Credit Card Fraud Analysis",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nFirst, we import the libraries required to perform the initial data analysis. The dataset, hosted on Kaggle, is imported and stored in the variable called credit_card.\n\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/download?datasetVersionNumber=3' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as credit_file:\n    credit_file.extractall(path='datasets')\n    \ncredit_card = pd.read_csv(Path('datasets/creditcard.csv')) # Store the dataset in a variable"
  },
  {
    "objectID": "posts/Anomaly detection/index.html#analyzing-the-data",
    "href": "posts/Anomaly detection/index.html#analyzing-the-data",
    "title": "Anomaly Detection: Credit Card Fraud Analysis",
    "section": "Analyzing the data",
    "text": "Analyzing the data\nLet us take a glance at the dataset using the head() method.\n\ncredit_card.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\nLet us explore further by using the describe() method. This gives us an idea of the distribution of the column values.\n\ncredit_card.describe()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\ncount\n284807.000000\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n...\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n284807.000000\n284807.000000\n\n\nmean\n94813.859575\n1.168375e-15\n3.416908e-16\n-1.379537e-15\n2.074095e-15\n9.604066e-16\n1.487313e-15\n-5.556467e-16\n1.213481e-16\n-2.406331e-15\n...\n1.654067e-16\n-3.568593e-16\n2.578648e-16\n4.473266e-15\n5.340915e-16\n1.683437e-15\n-3.660091e-16\n-1.227390e-16\n88.349619\n0.001727\n\n\nstd\n47488.145955\n1.958696e+00\n1.651309e+00\n1.516255e+00\n1.415869e+00\n1.380247e+00\n1.332271e+00\n1.237094e+00\n1.194353e+00\n1.098632e+00\n...\n7.345240e-01\n7.257016e-01\n6.244603e-01\n6.056471e-01\n5.212781e-01\n4.822270e-01\n4.036325e-01\n3.300833e-01\n250.120109\n0.041527\n\n\nmin\n0.000000\n-5.640751e+01\n-7.271573e+01\n-4.832559e+01\n-5.683171e+00\n-1.137433e+02\n-2.616051e+01\n-4.355724e+01\n-7.321672e+01\n-1.343407e+01\n...\n-3.483038e+01\n-1.093314e+01\n-4.480774e+01\n-2.836627e+00\n-1.029540e+01\n-2.604551e+00\n-2.256568e+01\n-1.543008e+01\n0.000000\n0.000000\n\n\n25%\n54201.500000\n-9.203734e-01\n-5.985499e-01\n-8.903648e-01\n-8.486401e-01\n-6.915971e-01\n-7.682956e-01\n-5.540759e-01\n-2.086297e-01\n-6.430976e-01\n...\n-2.283949e-01\n-5.423504e-01\n-1.618463e-01\n-3.545861e-01\n-3.171451e-01\n-3.269839e-01\n-7.083953e-02\n-5.295979e-02\n5.600000\n0.000000\n\n\n50%\n84692.000000\n1.810880e-02\n6.548556e-02\n1.798463e-01\n-1.984653e-02\n-5.433583e-02\n-2.741871e-01\n4.010308e-02\n2.235804e-02\n-5.142873e-02\n...\n-2.945017e-02\n6.781943e-03\n-1.119293e-02\n4.097606e-02\n1.659350e-02\n-5.213911e-02\n1.342146e-03\n1.124383e-02\n22.000000\n0.000000\n\n\n75%\n139320.500000\n1.315642e+00\n8.037239e-01\n1.027196e+00\n7.433413e-01\n6.119264e-01\n3.985649e-01\n5.704361e-01\n3.273459e-01\n5.971390e-01\n...\n1.863772e-01\n5.285536e-01\n1.476421e-01\n4.395266e-01\n3.507156e-01\n2.409522e-01\n9.104512e-02\n7.827995e-02\n77.165000\n0.000000\n\n\nmax\n172792.000000\n2.454930e+00\n2.205773e+01\n9.382558e+00\n1.687534e+01\n3.480167e+01\n7.330163e+01\n1.205895e+02\n2.000721e+01\n1.559499e+01\n...\n2.720284e+01\n1.050309e+01\n2.252841e+01\n4.584549e+00\n7.519589e+00\n3.517346e+00\n3.161220e+01\n3.384781e+01\n25691.160000\n1.000000\n\n\n\n\n8 rows × 31 columns\n\n\n\nLet us look at the non-null values using the info() method.\n\ncredit_card.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n\n\nWe can see that there are no null values in the dataset and all the classes are either integers or float values. Let us also look at how the classes are distributed.\n\ncredit_card.Class.value_counts()\n\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\n\nAs we can see, the dataset has a high degree of class imbalance. Therefore, we have to scale the features such that they are robust to outliers. Thus, we will import the RobustScaler class.\n\nfrom sklearn.preprocessing import RobustScaler\n\ncolumns = [columns for columns in credit_card.columns if columns not in ['Time','Class']] # columns to scale w.r.t outliers\n\nfor cols in columns:\n  robust_scaler = RobustScaler()\n  credit_card[cols] = robust_scaler.fit_transform(credit_card[cols].values.reshape(-1,1)) # reshape the columns such that they are robust to outliers\n\nLet us now see the linear correlations between various features.\n\ncorr = credit_card.corr() # correlation matrix\n\nfig, ax = plt.subplots(figsize=(50,50))\nsns.heatmap(corr, annot=True, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)\nplt.title('Linear Correlations between attributes')\nplt.show()\n\n\n\n\nAs we can see, there is almost no correlation between V1,….,V28. There is some negative correlation between Amount and V2, but it is not really significant.\nNow, let us split the target and the feature columns in our dataset.\n\ncolumns = [columns for columns in credit_card.columns if columns not in ['Class']] # Select feature columns\n\nX = credit_card[columns] # feature columns\ny = credit_card['Class'] # target columns"
  },
  {
    "objectID": "posts/Anomaly detection/index.html#dimensionality-reduction",
    "href": "posts/Anomaly detection/index.html#dimensionality-reduction",
    "title": "Anomaly Detection: Credit Card Fraud Analysis",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nWe will perform dimensionality reduction on this dataset by using Principal Component Analysis. We will do dimensionality reduction by taking only the 3 components.\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3, random_state=42)\n\nX_reduced = pca.fit_transform(X) # fit and transform the PCA\nX_reduced = pd.DataFrame(X_reduced,columns=['pca1','pca2','pca3']) # make a dataframe out of X_reduced\nX_reduced['Class'] = credit_card['Class'] # Add the Class feature to the PCA reduced dataset"
  },
  {
    "objectID": "posts/Anomaly detection/index.html#anomalyoutlier-detection",
    "href": "posts/Anomaly detection/index.html#anomalyoutlier-detection",
    "title": "Anomaly Detection: Credit Card Fraud Analysis",
    "section": "Anomaly/Outlier Detection",
    "text": "Anomaly/Outlier Detection\nSince this is a high-dimensional dataset, we can try using the IsolationForestalgorithm.\n\nfrom sklearn.ensemble import IsolationForest\n\nfrauds = credit_card[credit_card['Class'] == 1] # number of frauds\nvalids = credit_card[credit_card['Class'] == 0] # number of valid classes\nfraction_frauds = len(frauds)/len(credit_card) # fraction of frauds\n\nisf = IsolationForest(max_samples=len(credit_card),contamination=fraction_frauds,random_state=42) # Isolation forest\n\nisf.fit(X) # fit the randomized search \n\nIsolationForest(contamination=0.001727485630620034, max_samples=284807,\n                random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.IsolationForestIsolationForest(contamination=0.001727485630620034, max_samples=284807,\n                random_state=42)\n\n\nNow, let us view the accuracy_source of the Isolation Forest algorithm. The algorithm returns 1 if the instance is not an outlier. The algorithm returns -1 if the instance is an outlier. We need to replace these labels such that they match the Class attribute.\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = isf.predict(X) # Predictions from the Isolation Forest Algorithm\n\n# Reassign labels\n\ny_pred[y_pred == 1] = 0 # Instance is not an outlier\ny_pred[y_pred == -1] = 1 # Instance is an outlier\n\naccuracy_score(y_pred, y)\n\n0.9976194405334139\n\n\nHowever, due to the high class imbalance, we can’t be sure if the model is actually doing a good job. Let us see how many outliers the Isolation Forest algorithm correctly identified.\n\ncredit_card['Isolation_forest_pred'] = y_pred # Add a column for the predictions\ncredit_card['Isolation_forest_pred'].value_counts()\n\nIsolation_forest_pred\n0    284315\n1       492\nName: count, dtype: int64\n\n\nLet us compare the classes reported by the algorithm with the classes in the dataset.\n\ncredit_card[\"Class\"].value_counts()\n\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\n\nThe algorithm predicts the correct number of overall outliers. Let us now verify how many individual outliers the algorithm correctly predicted.\n\nlen(credit_card[(credit_card['Class'] == 1) & (credit_card['Isolation_forest_pred'] == 1)]) # check individual outliers which were predicted correctly \n\n153\n\n\nTherefore, only 153 outliers have been correctly identified. We can get a better sense of the predictions by looking at the confusion matrix below.\n\n# plot a confusion matrix for the dataset\n\nfrom sklearn.model_selection import cross_val_predict\n\ny_test_cm = y.copy() # use a new y with changed labels\ny_test_cm[y_test_cm == 1] = -1 # change labels to match those given by the Isolation Forest\ny_test_cm[y_test_cm == 0] = 1\n\n\n\ny_cm_pred = cross_val_predict(isf,X,y_test_cm,cv=3)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_cm, y_cm_pred)\ncm\n\narray([[   185,    307],\n       [   426, 283889]], dtype=int64)\n\n\nAs shown in the confusion matrix, the type I errors, i.e., 307 instances are classified as false positives (i.e., falsely classified as anomalies) across the three validation folds. Thus, the percentage of anomalies correctly predicted is only 37.6 %, whereas the accuracy score is 99.7 %. The precision and recall for this algorithm is computed by using the classification_report metric from scikit-learn.\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    284315\n           1       0.31      0.31      0.31       492\n\n    accuracy                           1.00    284807\n   macro avg       0.65      0.65      0.65    284807\nweighted avg       1.00      1.00      1.00    284807\n\n\n\nLet us now compare Isolation Forest with another algorithm, LocalOutlierFactor. The local outlier factor is an unsupervised anomaly detection algorithm which detects any deviation in the local density as compared to the neighbors.\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\nlco = LocalOutlierFactor(n_neighbors = 20, contamination = fraction_frauds) # local outlier\n\ny_local_pred = lco.fit_predict(X)\n\n# Reassign labels\n\ny_local_pred[y_local_pred == 1] = 0 # Instance is not an outlier\ny_local_pred[y_local_pred == -1] = 1 # Instance is an outlier\n\naccuracy_score(y_local_pred,y)\n\n0.9966152517318746\n\n\nNow, let us look at the classification report for this algorithm.\n\nprint(classification_report(y,y_local_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    284315\n           1       0.02      0.02      0.02       492\n\n    accuracy                           1.00    284807\n   macro avg       0.51      0.51      0.51    284807\nweighted avg       1.00      1.00      1.00    284807\n\n\n\nThis algorithm also produces a similar accuracy score (99.6 %) to the Isolation Forest. However, the precision and recall scores for the Local Outlier Fraction are much lower (2 %) as compared to 30 % for the Isolation Forest.\nThe poor precision and recall scores for both the algorithms are due to the high class imbalance in the dataset."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\nCS 5805 Machine Learning"
  }
]