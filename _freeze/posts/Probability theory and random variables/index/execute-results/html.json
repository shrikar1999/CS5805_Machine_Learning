{
  "hash": "4830bfe1959d93d44cd99dd8f4034556",
  "result": {
    "markdown": "---\ntitle: Probability Theory and Random Variables - Logistic Regression\nauthor: \"Shrikar Banagiri\"\ndate: \"2023-11-06\"\nimage: \"image.jpg\"\nexecute: \n  echo: false\n  freeze: true\n---\n\n## Introduction\n\nIn this blog, we will explore the probability theory underlying Logistic Regression. Unlike other Normal and Poisson Regression models, Logistic Regression is often used when the target labels are categorical. Thus, Logistic Regression is most suitable for classification problems. For example, let us imagine that we want to predict whether a patient is likely to have a heart attack or not. Therefore, the categorical model value Y :\n\n\n```{=tex}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{inputenc}\n\\usepackage{caption}\n\\usepackage{subcaption}\n```\n\n```{=tex}\n\\begin{equation}\n  Y = \n  \\begin{cases}\n    0,  no \\hspace{0.1cm} heart \\hspace{0.1cm} attack \\\\\n    1,  heart \\hspace{0.1cm} attack\n  \\end{cases}\n\\end{equation}\n```\n\nLet us assume that the target Y is dependent on features $X_1$, $X_2$, and $X_3$ which represent the patient age, patient weight, and cholesterol level of the patient.\n\n## Data Distribution Model\n\nIn our case, the target variable Y is binary, i.e., it can either take 0 or 1. Thus, we utilize the **Bernoulli probability distribution** model. The probability of patient *i* having a heart attack is denoted by *p~i~*\n\n$$\nY_{i}|p_{i} \\sim Bernoulli(p_{i})\n$${#eq-eqn1}\n\nwhere the expected or the mean value is given as\n\n$$\nE(Y_{i}|p_{i}) = p_{i}\n$$ {#eq-eqn2}\n\nThus, the expected value of the target variable Y is the probability of Y itself. Furthermore, this probability follows the Bernoulli distribution. Now, the final piece is to describe how the probability of patient *i* is related to the attributes $X_{1i}$, $X_{2i}$, and $X_{3i}$. For simplicity, let us assume, for now, that the probability of heart attack for patient *i* is only dependent on the feature $X_{1i}$. Let us also define a function $F(p_{i})$, which produces a linear relationship between the probability and the feature as shown below.\n\n$$\nF(p_{i}) = \\alpha + \\beta X_{1i}\n$$ {#eq-eqn3}\n\nThe right hand side of @eq-eqn3 is a linear function that can span the entire real space. Therefore, the left hand side also has to span the entire real space. Here's where we introduce the **logit** function. The logit function is defined as the logarithm of the odds of the probability distribution. Thus, the relationship becomes:\n\n$$\nF(p_{i}) = \\log{}(\\frac{p_{i}}{1 - p_{i}}) = \\alpha + \\beta X_{1i}\n$$ {#eq-eqn4}\n\nWhich on solving further, gives\n\n$$\nodds_{i} = \\frac{p_{i}}{1-p_{i}} = e^{\\alpha + \\beta X_{1i}}\n$$ {#eq-eqn5}\n\nSolving further,\n\n$$\np_{i} = \\frac{e^{\\alpha + \\beta X_{1i}}}{1 + e^{\\alpha + \\beta X_{1i}}}\n$$ {#eq-eqn6}\n\nLet us assume that the value of $\\alpha$ is - 4 and the value of $\\beta$ is 0.1 for our case. Using these values for our heart attack analysis, we can generate the following plots.\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=1183 height=302}\n:::\n:::\n\n\nThe figures show that even though the odds and the probability vary non-linearly with respect to the patient age, the logarithm of odds varies linearly with the patient age. In general, for multiple features $X_{1}$, $X_{2}$, ..., $X_{n}$, we can define the logarithm of odds to be:\n\n$$\n\\log{}(\\frac{p_{i}}{1 - p_{i}}) = \\alpha + \\beta X_{1} + \\beta_{2} X_2 + ... + \\beta_{n} X_{n}\n$$ {#eq-eqn7}\n\nEach new feature added has a multiplicative effect on the odds of heart attack, i.e., adding feature $X_{2}$ will result in multiplication by $e^{\\beta_{2}X_{2}}$. The coefficients $\\alpha$, $\\beta$, ..., $\\beta_{n}$ are estimated through a combination of our \"prior\" knowledge and \"posterior\" simulations on known datasets.\n\n## Classification Example\n\nIn this section, an example classification problem is solved using Logistic Regression. In this example, we will import the `iris` dataset. This is a classic dataset which is used in classification problems. The dataset contains three class, class '0' represents `setosa`, class '1' represents `versicolor`, and class '2' represents `virginica`. These classes depend on the features, `sepal length`, `sepal width`, `petal length`, and `petal width`.\n\nTo implement logistic regression on this dataset, let us first import the necessary libraries\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\niris = datasets.load_iris()\niris_data = iris.data # get data values\niris_target = iris.target # target labels\n\n# Display iris target labels\n\nprint(iris_target[iris_target == 0].shape) # print the number of setosa target datapoints\nprint(iris_target[iris_target == 1].shape) # print the number of versi-color target datapoints\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(50,)\n(50,)\n```\n:::\n:::\n\n\nTherefore, the number of setosa and versi-color target datapoints is 50 each. Let us now segregate the iris dataset such that we select only setosa and versi-color target variables. Furthermore, let us look at how the sepal length varies across target variables.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\niris_data = iris_data[0:100] # filter only setosa and versi-color target data\niris_target = iris_target[0:100] # filter only setosa and versi-color target labels\nplt.scatter(iris_data[:,0], iris_target)\nplt.xlabel('Sepal length (cm)')\nplt.ylabel('Target name')\nplt.grid()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=589 height=429}\n:::\n:::\n\n\nSince the target name is categorical and binary (either 0 or 1), we can see that the data is separated. Let us now import the `LogisticRegression` from scikit-learn. Logistic regression works by fitting a logit function to the separated data. If the value corresponding to a given sepal length falls below 0.5, then the model predicts the class as 0 (i.e. setosa). However, when the value corresponding to a given sepal length is greater than 0.5, then the model predicts the class as 1 (i.e., versi-color)\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.special import expit\n\nmodel = LogisticRegression(random_state=42)\nmodel.fit(iris_data[:,0].reshape(-1,1),iris_target)\n\n# Test the logistic regression model with a test sample\n\nsepal_sample = np.linspace(4.3,7,100) # a random sample of sepal lengths\ntarget_sample = sepal_sample.reshape(-1,1)*model.coef_ + model.intercept_\nlogistic_fit = expit(target_sample)\n\n# Plotting\nplt.scatter(iris_data[:,0],iris_target, c=iris_target,label = \"Sepal length\")\nplt.plot(sepal_sample,logistic_fit.ravel(),c='blue',label='Logistic regression')\nplt.axhline(.5, color=\"red\", label=\"Decision threshold\")\nplt.legend(loc='lower right')\nplt.grid()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=571 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}