{
  "hash": "dfa38d189e126c19614fcedc729d6463",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection: Credit Card Fraud Analysis \"\nauthor: \"Shrikar Banagiri\"\ndate: \"2023-11-28\"\nimage: \"image.jpg\"\nexecute: \n  echo: false\n  freeze: true\n---\n\n## Introduction\n\nThe [credit card fraud detection dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data) comprises of all the transactions made by credit card holders in the European Union in September 2013. Out of the total 284,807 transactions that have taken place, only 492 transactions are fraudulent. The dataset contains the following features: `Time`, which indicates the number of transactions that elapsed between the current transaction and the first transaction in the dataset, `V1`, ..., `V28` are all anonymous features obtained through principal component analysis. `Amount` indicates the transaction amount and `Class` is the outcome which set to 1 in case of fraud and 0 in case there is no fraud.\n\n## Importing the dataset\n\nFirst, we import the libraries required to perform the initial data analysis. The dataset, hosted on [Kaggle](https://www.kaggle.com/), is imported and stored in the variable called `credit_card`.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/download?datasetVersionNumber=3' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as credit_file:\n    credit_file.extractall(path='datasets')\n    \ncredit_card = pd.read_csv(Path('datasets/creditcard.csv')) # Store the dataset in a variable\n```\n:::\n\n\n## Analyzing the data\n\nLet us take a glance at the dataset using the `head()` method.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncredit_card.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\nLet us explore further by using the `describe()` method. This gives us an idea of the distribution of the column values.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ncredit_card.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94813.859575</td>\n      <td>1.168375e-15</td>\n      <td>3.416908e-16</td>\n      <td>-1.379537e-15</td>\n      <td>2.074095e-15</td>\n      <td>9.604066e-16</td>\n      <td>1.487313e-15</td>\n      <td>-5.556467e-16</td>\n      <td>1.213481e-16</td>\n      <td>-2.406331e-15</td>\n      <td>...</td>\n      <td>1.654067e-16</td>\n      <td>-3.568593e-16</td>\n      <td>2.578648e-16</td>\n      <td>4.473266e-15</td>\n      <td>5.340915e-16</td>\n      <td>1.683437e-15</td>\n      <td>-3.660091e-16</td>\n      <td>-1.227390e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\nLet us look at the non-null values using the `info()` method.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ncredit_card.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n```\n:::\n:::\n\n\nWe can see that there are no null values in the dataset and all the classes are either integers or float values. Let us also look at how the classes are distributed.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncredit_card.Class.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nClass\n0    284315\n1       492\nName: count, dtype: int64\n```\n:::\n:::\n\n\nAs we can see, the dataset has a high degree of **class imbalance**. Therefore, we have to scale the features such that they are robust to outliers. Thus, we will import the `RobustScaler` class.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import RobustScaler\n\ncolumns = [columns for columns in credit_card.columns if columns not in ['Time','Class']] # columns to scale w.r.t outliers\n\nfor cols in columns:\n  robust_scaler = RobustScaler()\n  credit_card[cols] = robust_scaler.fit_transform(credit_card[cols].values.reshape(-1,1)) # reshape the columns such that they are robust to outliers\n```\n:::\n\n\nLet us now see the linear correlations between various features.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncorr = credit_card.corr() # correlation matrix\n\nfig, ax = plt.subplots(figsize=(50,50))\nsns.heatmap(corr, annot=True, cmap='YlGnBu', vmin=-1, vmax=1, center=0, ax=ax)\nplt.title('Linear Correlations between attributes')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=3439 height=3758}\n:::\n:::\n\n\nAs we can see, there is almost no correlation between `V1`,....,`V28`. There is some negative correlation between `Amount` and `V2`, but it is not really significant.\n\nNow, let us split the target and the feature columns in our dataset.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ncolumns = [columns for columns in credit_card.columns if columns not in ['Class']] # Select feature columns\n\nX = credit_card[columns] # feature columns\ny = credit_card['Class'] # target columns\n```\n:::\n\n\n## Dimensionality Reduction\n\nWe will perform dimensionality reduction on this dataset by using Principal Component Analysis. We will do dimensionality reduction by taking only the 3 components.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3, random_state=42)\n\nX_reduced = pca.fit_transform(X) # fit and transform the PCA\nX_reduced = pd.DataFrame(X_reduced,columns=['pca1','pca2','pca3']) # make a dataframe out of X_reduced\nX_reduced['Class'] = credit_card['Class'] # Add the Class feature to the PCA reduced dataset \n```\n:::\n\n\n## Anomaly/Outlier Detection\n\nSince this is a high-dimensional dataset, we can try using the `IsolationForest`algorithm.\n\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.ensemble import IsolationForest\n\nfrauds = credit_card[credit_card['Class'] == 1] # number of frauds\nvalids = credit_card[credit_card['Class'] == 0] # number of valid classes\nfraction_frauds = len(frauds)/len(credit_card) # fraction of frauds\n\nisf = IsolationForest(max_samples=len(credit_card),contamination=fraction_frauds,random_state=42) # Isolation forest\n\nisf.fit(X) # fit the randomized search \n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.001727485630620034, max_samples=284807,\n                random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.001727485630620034, max_samples=284807,\n                random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nNow, let us view the accuracy_source of the Isolation Forest algorithm. The algorithm returns 1 if the instance is not an outlier. The algorithm returns -1 if the instance is an outlier. We need to replace these labels such that they match the `Class` attribute.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score\n\ny_pred = isf.predict(X) # Predictions from the Isolation Forest Algorithm\n\n# Reassign labels\n\ny_pred[y_pred == 1] = 0 # Instance is not an outlier\ny_pred[y_pred == -1] = 1 # Instance is an outlier\n\naccuracy_score(y_pred, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n0.9976194405334139\n```\n:::\n:::\n\n\nHowever, due to the high class imbalance, we can't be sure if the model is actually doing a good job. Let us see how many outliers the Isolation Forest algorithm correctly identified.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ncredit_card['Isolation_forest_pred'] = y_pred # Add a column for the predictions\ncredit_card['Isolation_forest_pred'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nIsolation_forest_pred\n0    284315\n1       492\nName: count, dtype: int64\n```\n:::\n:::\n\n\nLet us compare the classes reported by the algorithm with the classes in the dataset.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ncredit_card[\"Class\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nClass\n0    284315\n1       492\nName: count, dtype: int64\n```\n:::\n:::\n\n\nThe algorithm predicts the correct number of overall outliers. Let us now verify how many individual outliers the algorithm correctly predicted.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nlen(credit_card[(credit_card['Class'] == 1) & (credit_card['Isolation_forest_pred'] == 1)]) # check individual outliers which were predicted correctly \n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n153\n```\n:::\n:::\n\n\nTherefore, only 153 outliers have been correctly identified. We can get a better sense of the predictions by looking at the confusion matrix below.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# plot a confusion matrix for the dataset\n\nfrom sklearn.model_selection import cross_val_predict\n\ny_test_cm = y.copy() # use a new y with changed labels\ny_test_cm[y_test_cm == 1] = -1 # change labels to match those given by the Isolation Forest\ny_test_cm[y_test_cm == 0] = 1\n\n\n\ny_cm_pred = cross_val_predict(isf,X,y_test_cm,cv=3)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_cm, y_cm_pred)\ncm\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([[   185,    307],\n       [   426, 283889]], dtype=int64)\n```\n:::\n:::\n\n\nAs shown in the confusion matrix, the **type I** errors, i.e., 307 instances are classified as false positives (i.e., falsely classified as anomalies) across the three validation folds. Thus, the percentage of anomalies correctly predicted is only **37.6 %**, whereas the accuracy score is **99.7 %**. The precision and recall for this algorithm is computed by using the `classification_report` metric from scikit-learn.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y,y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    284315\n           1       0.31      0.31      0.31       492\n\n    accuracy                           1.00    284807\n   macro avg       0.65      0.65      0.65    284807\nweighted avg       1.00      1.00      1.00    284807\n\n```\n:::\n:::\n\n\nLet us now compare Isolation Forest with another algorithm, `LocalOutlierFactor`. The local outlier factor is an unsupervised anomaly detection algorithm which detects any deviation in the local density as compared to the neighbors.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nfrom sklearn.neighbors import LocalOutlierFactor\n\nlco = LocalOutlierFactor(n_neighbors = 20, contamination = fraction_frauds) # local outlier\n\ny_local_pred = lco.fit_predict(X)\n\n# Reassign labels\n\ny_local_pred[y_local_pred == 1] = 0 # Instance is not an outlier\ny_local_pred[y_local_pred == -1] = 1 # Instance is an outlier\n\naccuracy_score(y_local_pred,y)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n0.9966152517318746\n```\n:::\n:::\n\n\nNow, let us look at the classification report for this algorithm.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nprint(classification_report(y,y_local_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    284315\n           1       0.02      0.02      0.02       492\n\n    accuracy                           1.00    284807\n   macro avg       0.51      0.51      0.51    284807\nweighted avg       1.00      1.00      1.00    284807\n\n```\n:::\n:::\n\n\nThis algorithm also produces a similar accuracy score (**99.6 %**) to the Isolation Forest. However, the precision and recall scores for the Local Outlier Fraction are much lower (**2 %**) as compared to 30 % for the Isolation Forest.\n\nThe poor precision and recall scores for both the algorithms are due to the high class imbalance in the dataset.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}