{
  "hash": "55339e1aaa688b43f06e7a2934370375",
  "result": {
    "markdown": "---\ntitle: Clustering of Customer Personality Analysis Data\nauthor: \"Shrikar Banagiri\"\ndate: \"2023-11-24\"\nimage: \"image.jpg\"\nexecute: \n  echo: false\n  freeze: true\n---\n\n## Introduction\n\nThe [Customer Personality Analysis dataset](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis) contains various different features to aid a company in customer segmentation. Customer segmentation is used to cluster customers into different categories based on their demographic, lifestyle, behavior, and so on. The dataset, which is hosted on [Kaggle](https://www.kaggle.com/), has the following features:\n\n-   `Year_Birth`: Customer's year of birth.\n\n-   `Education`: Customer's education level.\n\n-   `Marital_Status`: Customer's marital status.\n\n-   `Income`: Customer's yearly household income.\n\n-   `Kidhome`: Number of kids at the customer's home.\n\n-   `Teenhome`: Number of teenagers at the customer's home.\n\n-   `Dt_Customer`: Date on which the customer enrolled with the company.\n\n-   `Recency`: Number of days since the customer's last purchase.\n\n-   `MntWines`: Amount of money spent on wines in the last two years.\n\n-   `MntFruits`: Amount of money spent on fruits in the last two years.\n\n-   `MntMeatProducts`: Amount of money spent on meat in the last two years.\n\n-   `MntFishProducts`: Amount of money spent on fish in the last two years.\n\n-   `MntSweetProducts`: Amount of money spent on sweets in the last two years.\n\n-   `MntGoldProds`: Amount of money spent on gold in the last two years.\n\n-   `NumDealsPurchases`: Number of purchases made with a discount\n\n-   `AcceptedCmp1`: 1 if the customer accepted the offer in the 1st campaign, 0 otherwise\n\n-   `AcceptedCmp2`: 1 if the customer accepted the offer in the 2nd campaign, 0 otherwise\n\n-   `AcceptedCmp3`: 1 if the customer accepted the offer in the 3rd campaign, 0 otherwise\n\n-   `AcceptedCmp4`: 1 if the customer accepted the offer in the 4th campaign, 0 otherwise\n\n-   `AcceptedCmp5`: 1 if the customer accepted the offer in the 5th campaign, 0 otherwise\n\n-   `Response`: 1 if the customer accepted the offer in the last campaign, 0 otherwise\n\nGiven these features, we want to perform clustering to split the data into customer segments.\n\n## Importing the dataset\n\nFirst, we import the libraries required to perform the initial data analysis. Next, let us store the dataset in a new variable, `customer_data`.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport urllib.request\nfrom pathlib import Path\nimport os\nimport zipfile\n\n# Downloading and opening the dataset\n\ncsv_path = Path('datasets/archive.zip') # store the dataset in a local folder\nurl = 'https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis/download?datasetVersionNumber=1' # url to download the dataset\n\nif not csv_path.is_file(): # check if the dataset directory exists. \n  Path(\"datasets\").mkdir(parents=True,exist_ok=True) # Create the directory\n  urllib.request.urlretrieve(url, csv_path)\n  with zipfile.ZipFile(csv_path) as customer_file:\n    customer_file.extractall(path='datasets')\n    \ncustomer_data = pd.read_csv(Path('datasets/marketing_campaign.csv')) # Store the dataset in a variable\n```\n:::\n\n\n## Analyzing the Data\n\nLet us look at the dataset by using the `head()` method.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncustomer_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Year_Birth</th>\n      <th>Education</th>\n      <th>Marital_Status</th>\n      <th>Income</th>\n      <th>Kidhome</th>\n      <th>Teenhome</th>\n      <th>Dt_Customer</th>\n      <th>Recency</th>\n      <th>MntWines</th>\n      <th>...</th>\n      <th>NumWebVisitsMonth</th>\n      <th>AcceptedCmp3</th>\n      <th>AcceptedCmp4</th>\n      <th>AcceptedCmp5</th>\n      <th>AcceptedCmp1</th>\n      <th>AcceptedCmp2</th>\n      <th>Complain</th>\n      <th>Z_CostContact</th>\n      <th>Z_Revenue</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5524</td>\n      <td>1957</td>\n      <td>Graduation</td>\n      <td>Single</td>\n      <td>58138.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>04-09-2012</td>\n      <td>58</td>\n      <td>635</td>\n      <td>...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2174</td>\n      <td>1954</td>\n      <td>Graduation</td>\n      <td>Single</td>\n      <td>46344.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>08-03-2014</td>\n      <td>38</td>\n      <td>11</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4141</td>\n      <td>1965</td>\n      <td>Graduation</td>\n      <td>Together</td>\n      <td>71613.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21-08-2013</td>\n      <td>26</td>\n      <td>426</td>\n      <td>...</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6182</td>\n      <td>1984</td>\n      <td>Graduation</td>\n      <td>Together</td>\n      <td>26646.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>10-02-2014</td>\n      <td>26</td>\n      <td>11</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5324</td>\n      <td>1981</td>\n      <td>PhD</td>\n      <td>Married</td>\n      <td>58293.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19-01-2014</td>\n      <td>94</td>\n      <td>173</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 29 columns</p>\n</div>\n```\n:::\n:::\n\n\nLet's look at the non-null entries in the dataset using the `info()` method below.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ncustomer_data.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2240 entries, 0 to 2239\nData columns (total 29 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   ID                   2240 non-null   int64  \n 1   Year_Birth           2240 non-null   int64  \n 2   Education            2240 non-null   object \n 3   Marital_Status       2240 non-null   object \n 4   Income               2216 non-null   float64\n 5   Kidhome              2240 non-null   int64  \n 6   Teenhome             2240 non-null   int64  \n 7   Dt_Customer          2240 non-null   object \n 8   Recency              2240 non-null   int64  \n 9   MntWines             2240 non-null   int64  \n 10  MntFruits            2240 non-null   int64  \n 11  MntMeatProducts      2240 non-null   int64  \n 12  MntFishProducts      2240 non-null   int64  \n 13  MntSweetProducts     2240 non-null   int64  \n 14  MntGoldProds         2240 non-null   int64  \n 15  NumDealsPurchases    2240 non-null   int64  \n 16  NumWebPurchases      2240 non-null   int64  \n 17  NumCatalogPurchases  2240 non-null   int64  \n 18  NumStorePurchases    2240 non-null   int64  \n 19  NumWebVisitsMonth    2240 non-null   int64  \n 20  AcceptedCmp3         2240 non-null   int64  \n 21  AcceptedCmp4         2240 non-null   int64  \n 22  AcceptedCmp5         2240 non-null   int64  \n 23  AcceptedCmp1         2240 non-null   int64  \n 24  AcceptedCmp2         2240 non-null   int64  \n 25  Complain             2240 non-null   int64  \n 26  Z_CostContact        2240 non-null   int64  \n 27  Z_Revenue            2240 non-null   int64  \n 28  Response             2240 non-null   int64  \ndtypes: float64(1), int64(25), object(3)\nmemory usage: 507.6+ KB\n```\n:::\n:::\n\n\nAs we can see in the dataset, the `Income` column has a few null values. Let us use the `SimpleImputer` class from scikit-learn to replace the null values with the column median.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')\n\ncustomer_data['Income'] = imputer.fit_transform(customer_data[['Income']]) # Replace the 'Income' column null values with median\n```\n:::\n\n\nNow, let us look at the customer data using the `info()` method.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncustomer_data.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2240 entries, 0 to 2239\nData columns (total 29 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   ID                   2240 non-null   int64  \n 1   Year_Birth           2240 non-null   int64  \n 2   Education            2240 non-null   object \n 3   Marital_Status       2240 non-null   object \n 4   Income               2240 non-null   float64\n 5   Kidhome              2240 non-null   int64  \n 6   Teenhome             2240 non-null   int64  \n 7   Dt_Customer          2240 non-null   object \n 8   Recency              2240 non-null   int64  \n 9   MntWines             2240 non-null   int64  \n 10  MntFruits            2240 non-null   int64  \n 11  MntMeatProducts      2240 non-null   int64  \n 12  MntFishProducts      2240 non-null   int64  \n 13  MntSweetProducts     2240 non-null   int64  \n 14  MntGoldProds         2240 non-null   int64  \n 15  NumDealsPurchases    2240 non-null   int64  \n 16  NumWebPurchases      2240 non-null   int64  \n 17  NumCatalogPurchases  2240 non-null   int64  \n 18  NumStorePurchases    2240 non-null   int64  \n 19  NumWebVisitsMonth    2240 non-null   int64  \n 20  AcceptedCmp3         2240 non-null   int64  \n 21  AcceptedCmp4         2240 non-null   int64  \n 22  AcceptedCmp5         2240 non-null   int64  \n 23  AcceptedCmp1         2240 non-null   int64  \n 24  AcceptedCmp2         2240 non-null   int64  \n 25  Complain             2240 non-null   int64  \n 26  Z_CostContact        2240 non-null   int64  \n 27  Z_Revenue            2240 non-null   int64  \n 28  Response             2240 non-null   int64  \ndtypes: float64(1), int64(25), object(3)\nmemory usage: 507.6+ KB\n```\n:::\n:::\n\n\nWe can now see that there are no null values in the dataset. Let us now look at the 'date' variables. Instead of using `Year_Birth`, perhaps it is better to use the customer age. Furthermore, let us convert the `Dt_Customer` feature to the 'datetime' format and create a `Num_Yrs_Customer` column which represents the number of years that the person has been a customer.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom datetime import date\nfrom datetime import datetime\n\ncustomer_data['Age'] = 2023 - customer_data['Year_Birth'] # Customer Age\ncustomer_data.drop('Year_Birth',axis=1,inplace=True) # Drop the Birth Year from the dataset\n\ncustomer_data['Dt_Customer'] = pd.to_datetime(customer_data['Dt_Customer'], format='%d-%m-%Y')\ncustomer_data['Num_Yrs_Customer'] = pd.Timestamp('now').year - customer_data['Dt_Customer'].dt.year # number of years that the person has been a customer\ncustomer_data.drop('Dt_Customer',axis=1,inplace=True) # Drop the Dt_Customer column\n```\n:::\n\n\nNow, let us do some more feature engineering by creating new columns. We observe that there are many different expenses such as `MntWines` , `MntFruits` , and so on. We will now add another column called `Total_Spending` which sums up the total expenditure for the past two years. We will create another column called `Num_Accept_Cmps` which adds up all the campaigns that have been accepted by the customer.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncustomer_data['Total_Spending'] = customer_data['MntFishProducts'] + customer_data['MntFruits'] + customer_data['MntGoldProds'] + customer_data['MntMeatProducts'] + customer_data['MntSweetProducts'] + customer_data['MntWines'] # create Total_Spending column\n\ncustomer_data['Num_Accept_Cmps'] = customer_data['AcceptedCmp1'] + customer_data['AcceptedCmp2'] + customer_data['AcceptedCmp3'] + customer_data['AcceptedCmp4'] + customer_data['AcceptedCmp5'] + customer_data['Response'] # create the Num_Accept_Cmps column\n```\n:::\n\n\nThe `Marital_Status` feature has many categories such as Married, Together, Single, Divorced, Widow, Alone, Absurd, and YOLO.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ncustomer_data.Marital_Status.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nMarital_Status\nMarried     864\nTogether    580\nSingle      480\nDivorced    232\nWidow        77\nAlone         3\nAbsurd        2\nYOLO          2\nName: count, dtype: int64\n```\n:::\n:::\n\n\nIt would be beneficial if we could reduce the number of categories by clubbing some of them together. For example, Alone, Absurd, Widow, Divorced, and YOLO would fall under the category of 'Alone' (which can be represented by 0). The rest of the columns can be replaced by 1. Furthermore, let us add a column called `Parent` whose value is 0 if there are no kids or teenagers. We will also add a column called `Family_members` which accounts for the total number of members in the household. Lastly, we will drop all the redundant columns from the dataset.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Replace redundant categories in marital status\n\ncustomer_data['Marital_Status'] = customer_data[\"Marital_Status\"].replace({\"Married\":1, \"Together\":1, \"Absurd\":0, \"Widow\":0, \"YOLO\":0, \"Divorced\":0, \"Single\":0,\"Alone\":0})\n\n# Add parent feature\n\ncustomer_data['Parent'] = np.where(customer_data.Kidhome + customer_data.Teenhome > 0, 1, 0)\n\n# Add Family members feature\n\ncustomer_data['Family_members'] = customer_data.Marital_Status.replace({0:1,1:2}) + customer_data.Kidhome + customer_data.Teenhome\n\n# Drop remaining unnecessary columns\n\ncustomer_data.drop(['Z_CostContact','Z_Revenue','ID'],axis = 1,inplace=True)\n```\n:::\n\n\nLet us look at the histograms of some of the features.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# plotting some of the features\n\nto_plot = ['Income','Recency','Age','Total_Spending'] \n\ncustomer_features = customer_data[to_plot]\ncustomer_features.hist(bins=50, figsize=(12, 12))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=947 height=949}\n:::\n:::\n\n\nFrom the histograms, we can observe that both `Income` and `Age` features have outliers that need to be removed.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Remove outliers from Income and Age\n\ncustomer_data = customer_data[(customer_data['Age'] < 100)] # Restrict Age values to below 100\ncustomer_data = customer_data[(customer_data['Income'] < 150000) ] # Restrict Income values to below 150000\n```\n:::\n\n\nNow, let us plot the histograms. As we can see, all the outliers have been removed.\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=947 height=949}\n:::\n:::\n\n\nNow, let us plot the correlation matrix for the dataset.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ncustomer_corr = customer_data.drop('Education',axis = 1) # Drop non-integer data\nfig, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(customer_corr.corr(), annot=True, cmap='YlGnBu', center=0, ax=ax)\nplt.title('Linear Correlations between attributes')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=1559 height=1677}\n:::\n:::\n\n\nAs we can see from the correlation matrix, there are multiple features which are highly correlated with other features. Therefore, we will do dimensionality reduction by using Principal Component Analysis.\n\nBut first, let us convert the remaining categorical variable Education to numerical values. To do this, let us use `LabelEncoder`.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import LabelEncoder\n\nlencoder = LabelEncoder() # label encoder\n\ncustomer_data['Education'] = lencoder.fit_transform(customer_data['Education'])\n```\n:::\n\n\nLet us standardize the features using `StandardScaler`.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\n\n# Segregate features which are supposed to be scaled\n\nto_scale = ['Income','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n'NumStorePurchases', 'NumWebVisitsMonth','Age', 'Num_Yrs_Customer', 'Total_Spending',\n'Num_Accept_Cmps', 'Parent', 'Family_members']\n\ncustomer_data[to_scale] = std_scaler.fit_transform(customer_data[to_scale]) # Scale the columns\n```\n:::\n\n\nLet us plot the histograms of the same representative features again.\n\n::: {.cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=947 height=949}\n:::\n:::\n\n\nWe can see that while the features `Income` and `Age` have gaussian like distributions and the recency feature is almost a perfect rectangle, the `Total_Spending` feature has a very long tail. Let us use the `boxcox` scaler to transform this feature.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nfrom scipy.stats import boxcox\n\ncustomer_data['Total_Spending'] = boxcox(customer_data['Total_Spending'],0) # Take the logarithm of the feature\n\n# plot histograms again\n\nto_plot = ['Income','Recency','Age','Total_Spending'] \n\ncustomer_features = customer_data[to_plot]\ncustomer_features.hist(bins=50, figsize=(12, 12))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){width=947 height=949}\n:::\n:::\n\n\nLet us remove the outliers from this `Total_Spending` feature.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Remove outliers from Total_Spending\n\ncustomer_data = customer_data[(customer_data['Total_Spending'] > -6)] # Restrict Total_Spending values to above -6\n```\n:::\n\n\nAs we can see now, the `Total_Spending` feature has a better-looking distribution.\n\n::: {.cell execution_count=19}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=938 height=949}\n:::\n:::\n\n\n## Dimensionality Reduction\n\nSince the dataset has many features, we choose to reduce the dimensionality of our dataset using Principal Component Analysis (PCA). We will import the PCA class from scikit-learn. The number of components will be determined using `RandomizedSearchCV` for each classifier.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Import the PCA class\n\nfrom sklearn.decomposition import PCA\n```\n:::\n\n\n## Clustering\n\nLet us first import the `KMeans` algorithm from scikit-learn. Next, let us use `RandomizedSearchCV` to find the best estimator with the optimal number of PCA components and KMeans clusters.\n\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\n\nkmeans_clf = make_pipeline(PCA(random_state=42),KMeans(random_state=42,n_init=10)) # Import pipeline for dimensionality reduction and clustering\nparam_distrib = {\"pca__n_components\":np.arange(3,20),\"kmeans__n_clusters\":np.arange(2,10)} # random parameter distribution\n\nrnd_search = RandomizedSearchCV(kmeans_clf, param_distrib, n_iter=10, cv=3,random_state=42)\n\nrnd_search.fit(customer_data)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=42)),\n                                             (&#x27;kmeans&#x27;,\n                                              KMeans(n_init=10,\n                                                     random_state=42))]),\n                   param_distributions={&#x27;kmeans__n_clusters&#x27;: array([2, 3, 4, 5, 6, 7, 8, 9]),\n                                        &#x27;pca__n_components&#x27;: array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])},\n                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=42)),\n                                             (&#x27;kmeans&#x27;,\n                                              KMeans(n_init=10,\n                                                     random_state=42))]),\n                   param_distributions={&#x27;kmeans__n_clusters&#x27;: array([2, 3, 4, 5, 6, 7, 8, 9]),\n                                        &#x27;pca__n_components&#x27;: array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])},\n                   random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=42)),\n                (&#x27;kmeans&#x27;, KMeans(n_init=10, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_init=10, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\nWe can now see what the best estimator is. As we can see, the optimal number of PCA components is 5 and optimal number of KMeans clusters is 3.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nrnd_search.best_estimator_\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=5, random_state=42)),\n                (&#x27;kmeans&#x27;, KMeans(n_clusters=3, n_init=10, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=5, random_state=42)),\n                (&#x27;kmeans&#x27;, KMeans(n_clusters=3, n_init=10, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=5, random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3, n_init=10, random_state=42)</pre></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\nTherefore, we will use these parameters henceforth.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nkm = KMeans(n_clusters=3, random_state=42,n_init=10) # Import KMeans clustering\npca = PCA(n_components=5, random_state=42) # Import PCA\n\ncustomer_data_reduced = pca.fit_transform(customer_data) # reduce pca components\nkm.fit(customer_data_reduced)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=3, n_init=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3, n_init=10, random_state=42)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nLet us look at the `silhouette_score` for the KMeans cluster.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(customer_data_reduced,km.labels_)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n0.26082278755573535\n```\n:::\n:::\n\n\nThe silhouette score for this clustering algorithm is around 0.26. Let us plot the silhouette diagram to investigate further.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# Plotting silhouette coefficients\n\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(customer_data_reduced) for k in range(1, 10)] # kmeans cluster\nsilhouette_scores = [silhouette_score(customer_data_reduced, model.labels_) for model in kmeans_per_k[1:]]\n\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(11, 9))\n\nfor k in (3, 4, 5, 6):\n    plt.subplot(2, 2, k - 2)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(customer_data_reduced, y_pred)\n\n    padding = len(customer_data_reduced) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (3, 5):\n        plt.ylabel(\"Cluster\")\n    \n    if k in (5, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"Silhouette Coefficient\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if k in (3, 4):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n  \n    plt.axvline(x=silhouette_scores[k], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\n    \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-27-output-1.png){width=884 height=745}\n:::\n:::\n\n\nThe knife edge plot shows that although k = 3 is the optimum number of clusters by performing randomized search, all the remaining clusters produce similar silhouette scores. Let's take a look at the elbow plot below, with the elbow shown at k = 3. As we can see from the plot, the inertia drops slowly if we increase k above 3.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ninertias = [model.inertia_ for model in kmeans_per_k]\n\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\n\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.axvline(x=4, color=\"red\", linestyle=\"--\")\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-28-output-1.png){width=610 height=429}\n:::\n:::\n\n\nNow, let us compare the performance of KMeans cluster with an Agglomerative cluster. The code is shown below.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nfrom sklearn.cluster import AgglomerativeClustering\n\n\nagc = AgglomerativeClustering(n_clusters=3)\nagc.fit(customer_data_reduced)\n\nsilhouette_score(customer_data_reduced,agc.labels_)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n0.23970062590927582\n```\n:::\n:::\n\n\nThe Agglomerative cluster does slightly worse than the KMeans cluster. Let us use the KMeans cluster to analyze the results.\n\n## Analyzing Results\n\nLet us plot a bar chart of the 4 clusters as evaluated by KMeans.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ny_pred = km.predict(customer_data_reduced) # Cluster predictions\ncustomer_data['cluster'] = y_pred # Assigning a new cluster column\n\n\nbar_plot = sns.countplot(x=customer_data[\"cluster\"])\nbar_plot.set_title('Cluster distribution')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-30-output-1.png){width=593 height=449}\n:::\n:::\n\n\nThe bar plot shows that most of the data is being clustered into 'Cluster 0' and the least amount of data is allocated to 'Cluster 2'. Otherwise, the data seems to be well distributed. Let us also look at a scatterplot to look at the distribution of cluster.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# Cluster customers based on income and expenditure\n\n# Use inverse transform to restore original data\ncustomer_data['Total_Spending'] = np.exp(customer_data['Total_Spending']) # Inverse of boxcox\ncustomer_data[to_scale] = std_scaler.inverse_transform(customer_data[to_scale]) # inverse transform to restore initial variables\nscatter = sns.scatterplot(data=customer_data,x=customer_data['Total_Spending'],y=customer_data['Income'],hue=customer_data['cluster'])\n\nscatter.set_title('Customer clusters based on Income and Expenditure')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-31-output-1.png){width=619 height=449}\n:::\n:::\n\n\nFrom the scatterplot, we can make the following conclusions:\n\n-   Cluster '0' is on average, comprised of customers with intermediate income and intermediate total expenditure.\n\n-   Cluster '1' is on average, comprised of customers with low income and low total expenditure.\n\n-   Cluster '2' is on average, comprised of customers with high income and high total expenditure.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}